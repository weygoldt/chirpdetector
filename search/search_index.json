{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p> Chirpdetector </p> <p>     Detect communication signals of electric fish using deep neural networks \ud83d\udc1f\u26a1\ud83e\udde0      This project is still work in progress and will approximately be released in spring of 2024.  <p> Why? \ud83e\udd28 </p> <p>Chirps are by far the most thoroughly researched communication signal of electric, probably even all fish. But detecting chirps becomes hard when more than one fish is recorded. As a result, most of the research to date analyzes this signal in isolated individuals. This is not good.</p> <p>To tackle this isse, this package provides a simple toolbox to detect chirps of multiple fish on spectrograms. This enables true quantitative analyses of chirping between freely behaving fish for the first time.</p>"},{"location":"assingment/","title":"Assingment","text":"<p>Wow, such empty</p>"},{"location":"cli_reference/","title":"CLI Reference","text":"<p>This page provides the full reference of the chirpdetector command line interface.</p> <p>Instead of using <code>chirpdetector COMMAND</code>, all commands documented here can also be invoked with the much briefer <code>cpd COMMAND</code>.</p>"},{"location":"cli_reference/#chirpdetector","title":"chirpdetector","text":"<p>Welcome to Chirpdetector Version: 0.0.1 </p> <p>The chirpdetector command line tool is a collection of commands that     make it easier to detect chirps of wave-type weakly electric     fish on a spectrogram. The usual workflow is:</p> <pre><code>1. `copyconfig` to copy the configfile to your dataset.\n2. `convert` to convert your dataset to training data.\n3. label your data, e.g. in label-studio.\n4. `train` to train the detector.\n5. `detect` to detect chirps on your dataset.\n6. `assign` to assign detections to the tracks of individual fish.\n\nRepeat this cycle from (2) to (5) until you are satisfied with the\ndetection performance.\n\nFor more information including a tutorial, see the documentation at\n*https://weygoldt.com/chirpdetector*\n</code></pre> <p>Usage:</p> <pre><code>chirpdetector [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -V, --version  Show the version and exit.\n  --help         Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#assign","title":"assign","text":"<p>Detect chirps on a spectrogram.</p> <p>Usage:</p> <pre><code>chirpdetector assign [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --path DIRECTORY  Path to the dataset.  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#convert","title":"convert","text":"<p>Convert a wavetracker dataset to YOLO.</p> <p>Convert wavetracker dataset to labeled or unlabeled spectrogram images to train the model.</p> <p>Usage:</p> <pre><code>chirpdetector convert [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input_path DIRECTORY      Path to the input dataset.  [required]\n  -o, --output_path DIRECTORY     Path to the output dataset.  [required]\n  -l, --labels [none|synthetic|detected]\n                                  Whether labels are not there yet (none),\n                                  simulated (synthetic) or inferred by the\n                                  detector (detected).  [required]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#copyconfig","title":"copyconfig","text":"<p>Copy the default config file to your dataset.</p> <p>Usage:</p> <pre><code>chirpdetector copyconfig [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input_path DIRECTORY  Path to the dataset.  [required]\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#datautils","title":"datautils","text":"<p>Utilities to manage YOLO-style training datasets.</p> <p>Usage:</p> <pre><code>chirpdetector datautils [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#clean","title":"clean","text":"<p>Remove all images where the label file is empty.</p> <p>Usage:</p> <pre><code>chirpdetector datautils clean [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --path DIRECTORY  Path to the dataset.  [required]\n  -e, --img_ext TEXT    The image extension, e.g. .png or .jpg  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#merge","title":"merge","text":"<p>Merge two datasets.</p> <p>Usage:</p> <pre><code>chirpdetector datautils merge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -d1, --dataset1 DIRECTORY  Path to the first dataset.  [required]\n  -d2, --dataset2 DIRECTORY  Path to the second dataset.  [required]\n  -o, --output DIRECTORY     Path to the output dataset.  [required]\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#subset","title":"subset","text":"<p>Create a subset of a dataset.</p> <p>Useful for manually labeling a small subset.</p> <p>Usage:</p> <pre><code>chirpdetector datautils subset [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --path DIRECTORY  Path to the dataset.  [required]\n  -e, --img_ext TEXT    The image extension, e.g. .png or .jpg  [required]\n  -n, --n INTEGER       The size of the subset  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#detect","title":"detect","text":"<p>Detect chirps on a spectrogram.</p> <p>Usage:</p> <pre><code>chirpdetector detect [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --path DIRECTORY  Path to the dataset.  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#evaltrain","title":"evaltrain","text":"<p>Detect chirps on a spectrogram.</p> <p>Usage:</p> <pre><code>chirpdetector evaltrain [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --path DIRECTORY  Path to the dataset.  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#plot","title":"plot","text":"<p>Plot detected chirps on a spectrogram.</p> <p>You can supply a path to a single recording and plot all chirp detections for it or delete all plots if you supply the <code>--clean</code> option.</p> <p>Alternatively, you can supply a path to a folder containing multiple recordings and plot all chirp detections for all recordings if you supply the <code>--all</code> option. You can also delete all plots for all recordings if you supply the <code>--all</code> and <code>--clean</code> options.</p> <p>Usage:</p> <pre><code>chirpdetector plot [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --path DIRECTORY  Path to the dataset.  [required]\n  -a, --all             Whether to iterate over multiple datasets.\n  -c, --clean           Just delete plots in the current dataset.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli_reference/#train","title":"train","text":"<p>Train the model.</p> <p>Usage:</p> <pre><code>chirpdetector train [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --config_path FILE          Path to the configuration file.  [required]\n  -m, --mode [pretrain|finetune]  Whether to train the model with synthetic\n                                  data or to finetune a model with real data.\n                                  [required]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We are thrilled to have you join in making this project even better. Please feel free to browse through the resources and guidelines provided here, and let us know if there is anything specific you would like to contribute or discuss.</p> <p>If you would like to help to develop this package you can skim through the to-do list below as well as the contribution guidelines. Just fork the project, add your code and send a pull request. We are always happy to get some help  !</p> <p>If you encountered an issue using the <code>chirpdetector</code>, feel free to open an issue here.</p>"},{"location":"contributing/#contributors-guidelines","title":"Contributors guidelines","text":"<p>I try our best to adhere to good coding practices and catch up on writing tests for this package. As I am currently the only one working on it, here is some documentation of the development packages I use:</p> <ul> <li><code>pre-commit</code> for pre-commit hooks</li> <li><code>pytest</code> and <code>pytest-coverage</code> for unit tests</li> <li><code>ruff</code> for linting and formatting</li> <li><code>pyright</code> for static type checking</li> </ul> <p>Before every commit, a pre-commit hook runs all these packages on the code base and refuses a push if errors are raised. If you want to contribute, please make sure that your code is proberly formatted and run the tests before issuing a pull request. The formatting guidelines should be automatically picked up by your <code>ruff</code> installaton from the <code>pyproject.toml</code> file.</p>"},{"location":"contributing/#to-do","title":"To Do","text":"<p>After the first release, this section will be removed an tasks will be organized as github issues. Until them, if you fixed something, please check it off on this list before opening a pull request.</p> <ul> <li> Write an assingment benchmarking</li> <li> Write an assignmen GUI to create a ground truth</li> <li> Try a random forest classifier on PCAed envelope extractions to assign chirps</li> <li> Finish a script to analyze the Json dumps from the training loop</li> <li> Update all the docstrings after refactoring.</li> <li> Move hardcoded params from assignment algo into config.toml</li> <li> Split the messy training loop into functions.</li> <li> Remove all pyright warnings.</li> <li> Build github actions CI/CD pipeline for codecov etc.</li> <li> Implement multiprocessing in main detection loop: Compute a batch of       spectrograms parallely and pipe them all through the detector. And do this       simulatenously from multiple cores (if the GPU can receive tensors from multiple       cores). - Note: Multiprocessing increased the execution time due to the back and forth       between cpu and gpu (at least this is what google sais.) But I batched       detection at least.</li> <li> Check execution time for all the detect functions, got really slow after refactoring for some reason.</li> <li> Fix make test, fails after ruff run</li> <li> Refactor train, detect, convert. All into much smaller functions. Move accesory functions to utils</li> <li> Make complete codebase pass ruff</li> <li> Move the dataconverter from <code>gridtools</code> to <code>chirpdetector</code></li> <li> Extend the dataconverter to just output the spectrograms so that hand-labelling can be done in a separate step</li> <li> Add a main script so that the cli is <code>chirpdetector &lt;task&gt; --&lt;flag&gt; &lt;args&gt;</code></li> <li> Improve simulation of chirps to include more realistic noise, undershoot and maybe even phasic-tonic evolution of the frequency of the big chirps</li> <li> make the <code>copyconfig</code> script more</li> <li> start writing the chirp assignment algorithm</li> <li> Move all the pprinting and logging constructors to a separate module and build a unified console object so that saving logs to file is easier, also log to file as well</li> <li> Add label-studio</li> <li> Supply scripts to convert completely unannotated or partially annotated data to the label-studio format to make manual labeling easier</li> <li> Make possible to output detections as a yolo dataset</li> <li> Look up how to convert a yolo dataset to a label-studio input so we can label pre-annotated data, facilitating a full human-in-the-loop approach</li> <li> Add augmentation transforms to the dataset class and add augmentations to the simulation in <code>gridtools</code>. Note to this: Unnessecary, using real data.</li> <li> Change bbox to actual yolo format, not the weird one I made up (which is x1, y1, x2, y2 instead of x1, y1, w, h). This is why the label-studio export is not working.</li> <li> Port cli to click, works better</li> <li> Try clustering the detected chirp windows on a spectrogram, could be interesting</li> </ul>"},{"location":"data_structure/","title":"Data structure","text":"<p>To detect chirps on a recording of single or multiple electrodes, the dataset must fulfill the following requirements:</p> <ul> <li>The dataset is a single directory containing subdirectories for each single   <code>.raw</code> or <code>.wav</code> file, i.e. for each recording.</li> <li>Each raw recording file, i.e. the <code>.raw</code> or <code>.wav</code> file is named <code>traces_grid1.{file_ext}</code>.</li> <li>The fundamental frequencies of the fish in the recordings must be already   tracked using the wavetracker project.</li> </ul> <p>An example directory structure could look like the following:</p> <pre><code>\ue5ff dataset_root\n\u251c\u2500\u2500 \ue615 chirpdetector.toml    # the chirpdetector config\n\u251c\u2500\u2500 \ue5ff 2019-11-25-09_59      # dir of first recording\n\u2502  \u251c\u2500\u2500 \uf15b fund_v.npy         # wavetracker\n\u2502  \u251c\u2500\u2500 \uf15b ident_v.npy        # wavetracker\n\u2502  \u251c\u2500\u2500 \uf15b idx_v.npy          # wavetracker\n\u2502  \u251c\u2500\u2500 \uf15b sign_v.npy         # wavetracker\n\u2502  \u251c\u2500\u2500 \uf15b times.npy          # wavetracker\n\u2502  \u2514\u2500\u2500 \uf001 traces_grid1.wav   # raw recording\n\u251c\u2500\u2500 \ue5ff 2019-11-26-09_54      # dir of second recording\n\u2502  \u251c\u2500\u2500 \uf15b fund_v.npy         # ...\n...\n</code></pre> <p>If these requirements are met, detecting chirps should be no issue.</p>"},{"location":"dataset/","title":"Creating a dataset","text":"<p>Wow, such empty </p>"},{"location":"demo/","title":"Detecting chirps with a few terminal commands","text":"<p>Once everything is set up correctly, detecting chirps is a breeze. The terminal utility can be called by <code>chirpdetector</code> or simply <code>cpd</code>.</p> <p>Simply run  <pre><code>cpd detect --path \"/path/to/dataset\"\n</code></pre> And the bounding boxes will be computed and saved to a <code>.csv</code> file. Then run  <pre><code>cpd assign --path \"/path/to/dataset\"\n</code></pre> to assing each detected chirp to a fundamental frequency of a fish. The results will be added to the <code>.csv</code> file in the dataset. To check if this went well,  you can run  <pre><code>cpd plot --path \"/path/to/dataset\"\n</code></pre> And the spectrograms, bounding boxes, and assigned chirps of all the detected  chirps will be plotted and saved as <code>.png</code> images into a subfolder of your dataset.</p> <p>The result will look something like this:</p> <p> 15 seconds of a recording containing two chirping fish with bounding boxes around chirps and dots indicating to which frequency they are assigned to.</p>"},{"location":"how_it_works/","title":"How it works","text":"<p> How? \ud83e\udd14 </p> <p>Chirps manifest as excursions in the electric organ discharge frequency. To discern the individual chirps in a recording featuring multiple fish separated solely by frequency, we delve into the frequency domain. This involves the computation of spectrograms, ensuring ample temporal resolution for chirp distinction and sufficient frequency resolution for fish differentiation. The outcome is a series of images.</p> <p>This framework facilitates the application of potent computer vision algorithms, such as a faster-R-CNN, for the detection of objects like chirps within these 'images.' Each chirp detection yields a bounding box, a motif echoed in the package's logo.</p> <p>Post-processing steps refine the results, assigning chirp times to the fundamental frequencies of each fish captured in the recording.</p> <p> Still not sold? Check out the demo \u00bb</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installing-python","title":"Installing Python","text":"<p>This package requires <code>python&gt;=3.11.5</code>, which at the time of publishing, is not installed on all systems. The easiest way of installing a specific python version isolated from your system environtment is using <code>pyenv</code>. Follow the installation instructions and come back.</p>"},{"location":"installation/#setting-the-local-python-interpreter","title":"Setting the local <code>python</code> interpreter","text":"<p>Navigate into your project root directory and run <code>pyenv local 3.11.5</code> to set the local python version to <code>3.11.5</code>.</p>"},{"location":"installation/#creating-a-virtual-environment","title":"Creating a virtual environment","text":"<p>Create a virtual environment using <code>python3 -m venv .venv</code>. Now activate it by running <code>source .venv/bin/activate</code>.</p> <p>Alternatively, you can create a virtual environment using <code>pyenv</code> directly, wich has the benefit that it is activated automatically as soon as you <code>cd</code> into the directory. You can do this by running <code>pyenv virtualenv 3.11.5 chirpdetection</code> and then running <code>pyenv local chirpdetection</code>.</p>"},{"location":"installation/#cloning-the-repository","title":"Cloning the repository","text":"<p>As this project, as well as one of its dependencies, is currently under development, it is not yet published on <code>pypi</code> and you have to install it from github. To do this, clone this repository, as well as the <code>gridtools</code> repository like so:</p> <pre><code>git clone https://github.com/weygoldt/chirpdetector chirpdetector\ngit clone https://github.com/weygoldt/gridtools gridtools\n</code></pre> <p>You should now have a <code>chirpdetector</code> and a <code>gridtools</code> subdirectory in your project directory.</p>"},{"location":"installation/#installing-with-pip","title":"Installing with <code>pip</code>","text":"<p>Now you just have to install the packages locally using pythons package manager <code>pip</code>. First install <code>gridtools</code> and then <code>chirpdetector</code>, otherwise chirpdetector will look for gridtools and exit as it is not installed locally.</p> <pre><code>cd gridtools &amp;&amp; pip install -e .\ncd ../chirpdetector &amp;&amp; pip install -e .\n</code></pre> <p>If both packages and their dependencies installed correctly, you are all set. You can now continue to training the detector if you want to train the neural network using your own dataset. In most cases, I recommend to start by downloading a pretrained model instead.</p>"},{"location":"installation/#downloading-the-model","title":"Downloading the model","text":"<p>Coming soon</p>"},{"location":"labeling/","title":"Labeling a dataset","text":"<p>Wow, such empty </p>"},{"location":"setup/","title":"Setup","text":"<p>Wow, such empty </p>"},{"location":"training/","title":"Training","text":"<p>Wow, such empty </p>"},{"location":"visualization/","title":"Visualization","text":"<p>Wow, such empty </p>"},{"location":"yolo-helpers/","title":"Helper commands","text":"<p>Wow, such empty </p>"},{"location":"api/assign_chirps/","title":"assign_chirps","text":"<p>Assign chirps detected on a spectrogram to wavetracker tracks.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.ChirpAssignmentData","title":"<code>ChirpAssignmentData</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Data needed for chirp assignment.</p> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>class ChirpAssignmentData(BaseModel):\n    \"\"\"Data needed for chirp assignment.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    bbox_index: np.ndarray\n    env_trough_times: np.ndarray\n    env_trough_prominences: np.ndarray\n    env_trough_distances: np.ndarray\n    env_trough_indices: np.ndarray\n    track_ids: np.ndarray\n    envs: np.ndarray\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.assign_chirps","title":"<code>assign_chirps(ad, chirp_df, data)</code>","text":"<p>Assign chirps to wavetracker tracks.</p> <p>This function uses the extracted envelope troughs to assign chirps to tracks. It computes a cost function that is high when the trough prominence is high and the distance to the chirp center is low. For each chirp, the track with the highest cost function value is chosen.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.assign_chirps--parameters","title":"Parameters","text":"<ul> <li><code>assign_data</code>: <code>dict</code>     Dictionary containing the data needed for assignment</li> <li><code>chirp_df</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes</li> <li><code>data</code>: <code>gridtools.datasets.Dataset</code>     Dataset object containing the data</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def assign_chirps(\n    ad: ChirpAssignmentData,\n    chirp_df: pd.DataFrame,\n    data: Dataset,\n) -&gt; None:\n    \"\"\"Assign chirps to wavetracker tracks.\n\n    This function uses the extracted envelope troughs to assign chirps to\n    tracks. It computes a cost function that is high when the trough prominence\n    is high and the distance to the chirp center is low. For each chirp, the\n    track with the highest cost function value is chosen.\n\n    Parameters\n    ----------\n    - `assign_data`: `dict`\n        Dictionary containing the data needed for assignment\n    - `chirp_df`: `pd.dataframe`\n        Dataframe containing the chirp bboxes\n    - `data`: `gridtools.datasets.Dataset`\n        Dataset object containing the data\n    \"\"\"\n    # compute cost function.\n    # this function is high when the trough prominence is high\n    # (-&gt; chirp with high contrast)\n    # and when the trough is close to the chirp center as detected by the\n    # r-cnn (-&gt; detected chirp is close to the actual chirp)\n    cost = ad.env_trough_prominences / ad.env_trough_distances**2\n\n    # set cost to zero for cases where no peak was found\n    cost[np.isnan(cost)] = 0\n\n    # for each chirp, choose the track where the cost is highest\n    # TODO: to avoid confusion make a cost function where high is good and low\n    # is bad. this is more like a \"gain function\"\n    chosen_tracks = []  # the assigned ids\n    chosen_env_times = []  # the times of the envelope troughs\n    chosen_chirp_envs = []  # here go the full envelopes of the chosen chirps\n    non_chosen_chirp_envs = []  # here go full envelopes of nonchosen chirps\n    for idx in np.unique(ad.bbox_index):\n        candidate_tracks = ad.track_ids[ad.bbox_index == idx]\n        candidate_costs = cost[ad.bbox_index == idx]\n        candidate_times = ad.env_trough_times[ad.bbox_index == idx]\n        candidate_envs = ad.envs[ad.bbox_index == idx, :]\n\n        if np.all(np.isnan(candidate_times)):\n            chosen_tracks.append(np.nan)\n            chosen_env_times.append(np.nan)\n            continue\n\n        chosen_index = np.argmax(candidate_costs)\n        non_chosen_indices = np.arange(len(candidate_costs)) != chosen_index\n\n        env_time = candidate_times[chosen_index]\n        chosen_env_times.append(env_time)\n        if np.isnan(env_time):\n            chosen_tracks.append(np.nan)\n            print(f\"candidate costs: {candidate_costs}\")\n            print(f\"candidate times: {candidate_times}\")\n        else:\n            chosen_tracks.append(candidate_tracks[chosen_index])\n\n        cenv = candidate_envs[chosen_index, :]\n        ncenv = candidate_envs[non_chosen_indices, :]\n\n        chosen_chirp_envs.append(cenv)\n        for env in ncenv:\n            if np.all(np.isnan(env)):\n                continue\n            non_chosen_chirp_envs.append(env)\n\n    # print('Finished assigning chirps')\n    #\n    # chosen_env = np.array(chosen_chirp_envs)\n    # print(chosen_env.shape)\n    #\n    # non_chosen_env = np.array(non_chosen_chirp_envs)\n    # print(non_chosen_env.shape)\n    #\n    # # TODO: Save envs do disk for plotting\n    # import matplotlib.pyplot as plt\n    # import matplotlib as mpl\n    # mpl.use(\"TkAgg\")\n    #\n    # fig, ax = plt.subplots(1,2)\n    # for env in chosen_env:\n    #     ax[0].plot(env, alpha=0.05, c=\"k\")\n    #\n    # for env in non_chosen_env:\n    #     ax[1].plot(env, c=\"k\", alpha=0.05)\n    # plt.show()\n    #\n    # store chosen tracks in chirp_df\n    chirp_df[\"assigned_track\"] = chosen_tracks\n\n    # store chirp time estimated from envelope trough in chirp_df\n    chirp_df[\"envelope_trough_time\"] = chosen_env_times\n\n    # save chirp_df\n    chirp_df.to_csv(data.path / \"chirpdetector_bboxes.csv\", index=False)\n\n    # save old format:\n    chosen_tracks = np.array(chosen_tracks)\n    chosen_env_times = np.array(chosen_env_times)\n    chosen_tracks = chosen_tracks[~np.isnan(chosen_tracks)].astype(int)\n    chosen_env_times = chosen_env_times[~np.isnan(chosen_env_times)].astype(\n        float\n    )\n    np.save(data.path / \"chirp_ids_rcnn.npy\", chosen_tracks)\n    np.save(data.path / \"chirp_times_rcnn.npy\", chosen_env_times)\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.assign_cli","title":"<code>assign_cli(path)</code>","text":"<p>Assign chirps to wavetracker tracks.</p> <p>this is the command line interface for the assign_chirps function.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.assign_cli--parameters","title":"Parameters","text":"<ul> <li><code>path</code>: <code>pathlib.path</code>     path to the directory containing the chirpdetector.toml file</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def assign_cli(path: pathlib.Path) -&gt; None:\n    \"\"\"Assign chirps to wavetracker tracks.\n\n    this is the command line interface for the assign_chirps function.\n\n    Parameters\n    ----------\n    - `path`: `pathlib.path`\n        path to the directory containing the chirpdetector.toml file\n    \"\"\"\n    if not path.is_dir():\n        msg = f\"{path} is not a directory\"\n        raise ValueError(msg)\n\n    if not (path / \"chirpdetector.toml\").is_file():\n        msg = f\"{path} does not contain a chirpdetector.toml file\"\n        raise ValueError(msg)\n\n    logger = make_logger(__name__, path / \"chirpdetector.log\")\n    recs = list(path.iterdir())\n    recs = [r for r in recs if r.is_dir()]\n\n    msg = f\"found {len(recs)} recordings in {path}, starting assignment\"\n    prog.console.log(msg)\n    logger.info(msg)\n\n    prog.console.rule(\"starting assignment\")\n    with prog:\n        task = prog.add_task(\"assigning chirps\", total=len(recs))\n        for rec in recs:\n            msg = f\"assigning chirps in {rec}\"\n            logger.info(msg)\n            prog.console.log(msg)\n\n            data = load(rec)\n            chirp_df = pd.read_csv(rec / \"chirpdetector_bboxes.csv\")\n            assign_data, chirp_df, data = extract_assignment_data(\n                data, chirp_df\n            )\n            assign_chirps(assign_data, chirp_df, data)\n            prog.update(task, advance=1)\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.clean_bboxes","title":"<code>clean_bboxes(data, chirp_df)</code>","text":"<p>Clean up the chirp bboxes.</p> <p>This is a collection of filters that remove bboxes that either overlap, are out of range or otherwise do not make sense.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.clean_bboxes--parameters","title":"Parameters","text":"<ul> <li><code>data</code>: <code>gridtools.datasets.Dataset</code>     Dataset object containing the data</li> <li><code>chirp_df</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes</li> </ul>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.clean_bboxes--returns","title":"Returns","text":"<ul> <li><code>chirp_df_tf</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes that overlap with the range</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def clean_bboxes(data: Dataset, chirp_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clean up the chirp bboxes.\n\n    This is a collection of filters that remove bboxes that\n    either overlap, are out of range or otherwise do not make sense.\n\n    Parameters\n    ----------\n    - `data`: `gridtools.datasets.Dataset`\n        Dataset object containing the data\n    - `chirp_df`: `pd.dataframe`\n        Dataframe containing the chirp bboxes\n\n    Returns\n    -------\n    - `chirp_df_tf`: `pd.dataframe`\n        Dataframe containing the chirp bboxes that overlap with the range\n    \"\"\"\n    # non-max suppression: remove all chirp bboxes that overlap with\n    # another more than threshold\n    pick_indices = non_max_suppression_fast(chirp_df, 0.5)\n    chirp_df_nms = chirp_df.loc[pick_indices, :]\n\n    # track filter: remove all chirp bboxes that do not overlap with\n    # the range spanned by the min and max of the wavetracker frequency tracks\n    minf = np.min(data.track.freqs).astype(float)\n    maxf = np.max(data.track.freqs).astype(float)\n    chirp_df = remove_bboxes_outside_range(chirp_df_nms, minf, maxf)\n\n    # sort chirps in df by time, i.e. t1\n    chirp_df = chirp_df.sort_values(by=\"t1\", ascending=True)\n\n    # compute chirp times, i.e. center of the bbox x axis\n    chirp_df[\"chirp_times\"] = np.mean(chirp_df[[\"t1\", \"t2\"]], axis=1)\n\n    return chirp_df\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.extract_assignment_data","title":"<code>extract_assignment_data(data, chirp_df)</code>","text":"<p>Get envelope troughs to determine chirp assignment.</p> <p>This algorigthm assigns chirps to wavetracker tracks by a series of steps:</p> <ol> <li>clean the chirp bboxes</li> <li>for each fish track, filter the signal on the best electrode</li> <li>find troughs in the envelope of the filtered signal</li> <li>compute the prominence of the trough and the distance to the chirp center</li> <li>compute a cost function that is high when the trough prominence is high and the distance to the chirp center is low</li> <li>compare the value of the cost function for each track and choose the track with the highest cost function value</li> </ol>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.extract_assignment_data--parameters","title":"Parameters","text":"<ul> <li><code>data</code>: <code>dataset</code>     Dataset object containing the data</li> <li><code>chirp_df</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def extract_assignment_data(\n    data: Dataset, chirp_df: pd.DataFrame\n) -&gt; Tuple[ChirpAssignmentData, pd.DataFrame, Dataset]:\n    \"\"\"Get envelope troughs to determine chirp assignment.\n\n    This algorigthm assigns chirps to wavetracker tracks by a series of steps:\n\n    1. clean the chirp bboxes\n    2. for each fish track, filter the signal on the best electrode\n    3. find troughs in the envelope of the filtered signal\n    4. compute the prominence of the trough and the distance to the chirp\n    center\n    5. compute a cost function that is high when the trough prominence is high\n    and the distance to the chirp center is low\n    6. compare the value of the cost function for each track and choose the\n    track with the highest cost function value\n\n    Parameters\n    ----------\n    - `data`: `dataset`\n        Dataset object containing the data\n    - `chirp_df`: `pd.dataframe`\n        Dataframe containing the chirp bboxes\n    \"\"\"\n    # clean the chirp bboxes\n    chirp_df = clean_bboxes(data, chirp_df)\n\n    array_len = len(chirp_df) * len(data.track.ids)\n    track_ids = np.concatenate(\n        [np.full(len(chirp_df), fish_id) for fish_id in data.track.ids]\n    )\n    bbox_index = np.concatenate(\n        [np.arange(len(chirp_df)) for _ in range(len(data.track.ids))]\n    )\n\n    ad = ChirpAssignmentData(\n        bbox_index=bbox_index,\n        track_ids=track_ids,\n        env_trough_times=np.full(array_len, np.nan),\n        env_trough_prominences=np.full(array_len, np.nan),\n        env_trough_distances=np.full(array_len, np.nan),\n        env_trough_indices=np.full(array_len, np.nan),\n        envs=np.full((array_len, 20001), np.nan),\n    )\n\n    for outer_idx, fish_id in enumerate(data.track.ids):\n        # get chirps, times and freqs and powers for this track\n        chirps = chirp_df.chirp_times.to_numpy()\n        time = data.track.times[\n            data.track.indices[data.track.idents == fish_id]\n        ]\n        freq = data.track.freqs[data.track.idents == fish_id]\n        powers = data.track.powers[data.track.idents == fish_id, :]\n\n        if len(time) == 0:\n            continue  # skip if no track is found\n\n        for inner_idx, chirp in enumerate(chirps):\n            # find the closest time, freq and power to the chirp time\n            closest_idx = np.argmin(np.abs(time - chirp))\n            best_electrode = np.argmax(powers[closest_idx, :]).astype(int)\n            second_best_electrode = np.argsort(powers[closest_idx, :])[-2]\n            best_freq = freq[closest_idx]\n\n            # check if chirp overlaps with track\n            f1 = chirp_df.f1.to_numpy()[inner_idx]\n            f2 = chirp_df.f2.to_numpy()[inner_idx]\n            f2 = f1 + (f2 - f1) * 0.5  # range is the lower half of the bbox\n\n            # if chirp does not overlap with track, skip this chirp\n            if (f1 &gt; best_freq) or (f2 &lt; best_freq):\n                continue\n\n            # determine start and stop index of time window on raw data\n            # using bounding box start and stop times of chirp detection\n            chirp_indices_on_raw_data = make_chirp_indices_on_raw_data(\n                chirp_df, data, inner_idx, chirp\n            )\n\n            # extract envelope troughs\n            troughs, proms, env = extract_envelope_trough(\n                data,\n                best_electrode,\n                second_best_electrode,\n                best_freq,\n                chirp_indices_on_raw_data,\n            )\n\n            # if no envelope troughs are found, skip this chirp\n            # append nan to chirp id and envelope trough time\n            if len(troughs) == 0:\n                continue\n\n            # compute index to closest peak to chirp center\n            distances = np.abs(\n                troughs\n                - (chirp_indices_on_raw_data[2] - chirp_indices_on_raw_data[0])\n            )\n            closest_trough_idx = np.argmin(distances)\n            trough_time = (\n                chirp_indices_on_raw_data[0] + troughs[closest_trough_idx]\n            ) / data.grid.samplerate\n\n            # store data in assignment data object\n            tot_idx = outer_idx * len(chirps) + inner_idx\n            ad.env_trough_times[tot_idx] = trough_time\n            ad.env_trough_prominences[tot_idx] = proms[closest_trough_idx]\n            ad.env_trough_distances[tot_idx] = (\n                distances[closest_trough_idx] + 1\n            )\n            ad.env_trough_indices[tot_idx] = troughs[closest_trough_idx]\n            center_env_start = len(ad.envs[0, :]) // 2 - len(env) // 2\n            center_env_stop = len(ad.envs[0, :]) // 2 + len(env) // 2 + 1\n\n            # if the envelope is even, add a nan to the end\n            # otherwise it cant be centered in the array\n            if len(env) % 2 == 0:\n                env = np.concatenate((env, np.array([np.nan])))\n\n            # center the env in the array and cut off the ends\n            # if it is larger\n            if len(env) &gt; len(ad.envs[0, :]):\n                env = env[\n                    len(env) // 2 - len(ad.envs[0, :]) // 2 : len(env) // 2\n                    + len(ad.envs[0, :]) // 2\n                    + 1\n                ]\n\n            if len(env) != center_env_stop - center_env_start:\n                print(\"env\", len(env))\n                print(\"start\", center_env_start)\n                print(\"stop\", center_env_stop)\n                print(\"diff\", center_env_stop - center_env_start)\n\n            ad.envs[\n                tot_idx,\n                center_env_start:center_env_stop,\n            ] = env\n\n    return (\n        ad,\n        chirp_df,\n        data,\n    )\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.extract_envelope_trough","title":"<code>extract_envelope_trough(data, best_electrode, second_best_electrode, best_freq, indices)</code>","text":"<p>Extract envelope troughs.</p> <p>Extracts a snippet from the raw data around the chirp time and computes the envelope of the bandpass filtered signal. Then finds the troughs in the envelope and computes their prominences.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.extract_envelope_trough--parameters","title":"Parameters","text":"<ul> <li><code>data</code>: <code>gridtools.datasets.Dataset</code>     Dataset object containing the data</li> <li><code>best_electrode</code>: <code>int</code>     Index of the best electrode</li> <li><code>second_best_electrode</code>: <code>int</code>     Index of the second best electrode</li> <li><code>best_freq</code>: <code>float</code>     Frequency of the chirp</li> <li><code>indices</code>: <code>Tuple[int, int, int]</code>     Tuple containing the start, center, stop indices of the chirp</li> </ul>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.extract_envelope_trough--returns","title":"Returns","text":"<ul> <li><code>peaks</code>: <code>np.ndarray</code>     Indices of the envelope troughs</li> <li><code>proms</code>: <code>np.ndarray</code>     Prominences of the envelope troughs</li> <li><code>env</code>: <code>np.ndarray</code>     Envelope of the filtered signal</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def extract_envelope_trough(\n    data: Dataset,\n    best_electrode: int,\n    second_best_electrode: int,\n    best_freq: float,\n    indices: Tuple[int, int, int],\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Extract envelope troughs.\n\n    Extracts a snippet from the raw data around the chirp time and computes\n    the envelope of the bandpass filtered signal. Then finds the troughs in\n    the envelope and computes their prominences.\n\n    Parameters\n    ----------\n    - `data`: `gridtools.datasets.Dataset`\n        Dataset object containing the data\n    - `best_electrode`: `int`\n        Index of the best electrode\n    - `second_best_electrode`: `int`\n        Index of the second best electrode\n    - `best_freq`: `float`\n        Frequency of the chirp\n    - `indices`: `Tuple[int, int, int]`\n        Tuple containing the start, center, stop indices of the chirp\n\n    Returns\n    -------\n    - `peaks`: `np.ndarray`\n        Indices of the envelope troughs\n    - `proms`: `np.ndarray`\n        Prominences of the envelope troughs\n    - `env`: `np.ndarray`\n        Envelope of the filtered signal\n    \"\"\"\n    start_idx, stop_idx, _ = indices\n\n    # determine bandpass cutoffs above and below baseline frequency\n    lower_f = best_freq - 15\n    upper_f = best_freq + 15\n\n    # get the raw signal on the 2 best electrodes and make differential\n    raw1 = data.grid.rec[start_idx:stop_idx, best_electrode]\n    raw2 = data.grid.rec[start_idx:stop_idx, second_best_electrode]\n    raw = raw1 - raw2\n\n    # bandpass filter the raw signal\n    raw_filtered = bandpass_filter(\n        raw,\n        data.grid.samplerate,\n        lower_f,\n        upper_f,\n    )\n\n    # compute the envelope of the filtered signal\n    env = envelope(\n        signal=raw_filtered,\n        samplerate=data.grid.samplerate,\n        cutoff_frequency=50,\n    )\n\n    # normalize the envelope using the amplitude of the raw signal\n    env = env / np.max(np.abs(raw))\n\n    # cut of the first and last 20% of the envelope\n    env[: int(0.25 * len(env))] = np.nan\n    env[int(0.75 * len(env)) :] = np.nan\n\n    # find troughs in the envelope and compute trough prominences\n    peaks, params = find_peaks(-env, prominence=1e-3)\n    proms = params[\"prominences\"]\n    return peaks, proms, env\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.make_chirp_indices_on_raw_data","title":"<code>make_chirp_indices_on_raw_data(chirp_df, data, idx, chirp)</code>","text":"<p>Make indices for the chirp window.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.make_chirp_indices_on_raw_data--parameters","title":"Parameters","text":"<ul> <li><code>chirp_df</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes</li> <li><code>data</code>: <code>gridtools.datasets.Dataset</code>     Dataset object containing the data</li> <li><code>idx</code>: <code>int</code>     Index of the chirp in the chirp_df</li> <li><code>chirp</code>: <code>float</code>     Chirp time</li> </ul>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.make_chirp_indices_on_raw_data--returns","title":"Returns","text":"<ul> <li><code>start_idx</code>: <code>int</code>     Start index of the chirp window</li> <li><code>stop_idx</code>: <code>int</code>     Stop index of the chirp window</li> <li><code>center_idx</code>: <code>int</code>     Center index of the chirp window</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def make_chirp_indices_on_raw_data(\n    chirp_df: pd.DataFrame, data: Dataset, idx: int, chirp: float\n) -&gt; Tuple[int, int, int]:\n    \"\"\"Make indices for the chirp window.\n\n    Parameters\n    ----------\n    - `chirp_df`: `pd.dataframe`\n        Dataframe containing the chirp bboxes\n    - `data`: `gridtools.datasets.Dataset`\n        Dataset object containing the data\n    - `idx`: `int`\n        Index of the chirp in the chirp_df\n    - `chirp`: `float`\n        Chirp time\n\n    Returns\n    -------\n    - `start_idx`: `int`\n        Start index of the chirp window\n    - `stop_idx`: `int`\n        Stop index of the chirp window\n    - `center_idx`: `int`\n        Center index of the chirp window\n    \"\"\"\n    # determine start and stop index of time window on raw data\n    # using bounding box start and stop times of chirp detection\n    diffr = chirp_df.t2.to_numpy()[idx] - chirp_df.t1.to_numpy()[idx]\n    t1 = chirp_df.t1.to_numpy()[idx] - 0.5 * diffr\n    t2 = chirp_df.t2.to_numpy()[idx] + 0.5 * diffr\n\n    if t1 &lt; 0:\n        t1 = 0\n\n    start_idx = int(np.round(t1 * data.grid.samplerate))\n    stop_idx = int(np.round(t2 * data.grid.samplerate))\n    center_idx = int(np.round(chirp * data.grid.samplerate))\n\n    return start_idx, stop_idx, center_idx\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.non_max_suppression_fast","title":"<code>non_max_suppression_fast(chirp_df, overlapthresh)</code>","text":"<p>Faster implementation of non-maximum suppression.</p> <p>To remove overlapping bounding boxes. Is a slightly modified version of https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/ .</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.non_max_suppression_fast--parameters","title":"Parameters","text":"<ul> <li><code>chirp_df</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes</li> <li><code>overlapthresh</code>: <code>float</code>     Threshold for overlap between bboxes</li> </ul>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.non_max_suppression_fast--returns","title":"Returns","text":"<ul> <li><code>pick</code>: <code>list</code>     List of indices of bboxes to keep</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def non_max_suppression_fast(\n    chirp_df: pd.DataFrame,\n    overlapthresh: float,\n) -&gt; list:\n    \"\"\"Faster implementation of non-maximum suppression.\n\n    To remove overlapping bounding boxes.\n    Is a slightly modified version of\n    https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n    .\n\n    Parameters\n    ----------\n    - `chirp_df`: `pd.dataframe`\n        Dataframe containing the chirp bboxes\n    - `overlapthresh`: `float`\n        Threshold for overlap between bboxes\n\n    Returns\n    -------\n    - `pick`: `list`\n        List of indices of bboxes to keep\n    \"\"\"\n    # convert boxes to list of tuples and then to numpy array\n    boxes = chirp_df[[\"t1\", \"f1\", \"t2\", \"f2\"]].to_numpy()\n\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n\n    # initialize the list of picked indexes\n    pick = []\n\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    area = (x2 - x1) * (y2 - y1)\n    idxs = np.argsort(y2)\n\n    # keep looping while some indexes still remain in the indexes\n    # list\n    while len(idxs) &gt; 0:\n        # grab the last index in the indexes list and add the\n        # index value to the list of picked indexes\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n\n        # find the largest (x, y) coordinates for the start of\n        # the bounding box and the smallest (x, y) coordinates\n        # for the end of the bounding box\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n\n        # compute the width and height of the bounding box\n        w = np.maximum(0, xx2 - xx1)\n        h = np.maximum(0, yy2 - yy1)\n\n        # compute the ratio of overlap (intersection over union)\n        overlap = (w * h) / area[idxs[:last]]\n\n        # delete all indexes from the index list that have\n        idxs = np.delete(\n            idxs,\n            np.concatenate(([last], np.where(overlap &gt; overlapthresh)[0])),\n        )\n        # return the indicies of the picked boxes\n    return pick\n</code></pre>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.remove_bboxes_outside_range","title":"<code>remove_bboxes_outside_range(chirp_dataframe, min_frequency, max_frequency)</code>","text":"<p>Remove chirp bboxes that do not overlap with frequency tracks.</p>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.remove_bboxes_outside_range--parameters","title":"Parameters","text":"<ul> <li><code>chirp_dataframe</code>: <code>pd.dataframe</code>     Dataframe containing the chirp bboxes</li> <li><code>min_frequency</code>: <code>float</code>     Minimum frequency of the range</li> <li><code>max_frequency</code>: <code>float</code>     Maximum frequency of the range</li> </ul>"},{"location":"api/assign_chirps/#chirpdetector.assign_chirps.remove_bboxes_outside_range--returns","title":"Returns","text":"<ul> <li><code>pd.dataframe</code>     Dataframe containing the chirp bboxes that overlap with the range</li> </ul> Source code in <code>chirpdetector/assign_chirps.py</code> <pre><code>def remove_bboxes_outside_range(\n    chirp_dataframe: pd.DataFrame,\n    min_frequency: float,\n    max_frequency: float,\n) -&gt; pd.DataFrame:\n    \"\"\"Remove chirp bboxes that do not overlap with frequency tracks.\n\n    Parameters\n    ----------\n    - `chirp_dataframe`: `pd.dataframe`\n        Dataframe containing the chirp bboxes\n    - `min_frequency`: `float`\n        Minimum frequency of the range\n    - `max_frequency`: `float`\n        Maximum frequency of the range\n\n    Returns\n    -------\n    - `pd.dataframe`\n        Dataframe containing the chirp bboxes that overlap with the range\n    \"\"\"\n    # remove all chirp bboxes that have no overlap with the range spanned by\n    # minf and maxf\n\n    # first build a box that spans the entire range\n    range_box = np.array(\n        [0, min_frequency, np.max(chirp_dataframe.t2), max_frequency]\n    )\n\n    # now compute the intersection between the range box and each chirp bboxes\n    # and keep only those that have an intersection area &gt; 0\n    chirp_df_tf = chirp_dataframe.copy()\n    intersection = chirp_df_tf.apply(\n        lambda row: (\n            max(0, min(row[\"t2\"], range_box[2]) - max(row[\"t1\"], range_box[0]))\n            * max(\n                0,\n                min(row[\"f2\"], range_box[3]) - max(row[\"f1\"], range_box[1]),\n            )\n        ),\n        axis=1,\n    )\n    return chirp_df_tf.loc[intersection &gt; 0, :]\n</code></pre>"},{"location":"api/convert_data/","title":"convert_data","text":"<p>Functions and classes for converting data.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.bboxes_from_simulated_chirps","title":"<code>bboxes_from_simulated_chirps(data, nfft)</code>","text":"<p>Make bounding boxes of simulated chirps using the chirp parameters.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.bboxes_from_simulated_chirps--parameters","title":"Parameters","text":"<ul> <li><code>data</code> : <code>Dataset</code>     The dataset to make bounding boxes for.</li> <li><code>nfft</code> : int     The number of samples in the FFT.</li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.bboxes_from_simulated_chirps--returns","title":"Returns","text":"<p><code>pandas.DataFrame</code>     A dataframe with the bounding boxes.</p> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def bboxes_from_simulated_chirps(data: Dataset, nfft: int) -&gt; pd.DataFrame:\n    \"\"\"Make bounding boxes of simulated chirps using the chirp parameters.\n\n    Parameters\n    ----------\n    - `data` : `Dataset`\n        The dataset to make bounding boxes for.\n    - `nfft` : int\n        The number of samples in the FFT.\n\n    Returns\n    -------\n    `pandas.DataFrame`\n        A dataframe with the bounding boxes.\n    \"\"\"\n    # Time padding is one NFFT window\n    pad_time = nfft / data.grid.samplerate\n\n    # Freq padding is fixed by the frequency resolution\n    freq_res = data.grid.samplerate / nfft\n    pad_freq = freq_res * 50\n\n    boxes = []\n    ids = []\n    for fish_id in data.track.ids:\n        freqs = data.track.freqs[data.track.idents == fish_id]\n        times = data.track.times[\n            data.track.indices[data.track.idents == fish_id]\n        ]\n        chirps = data.com.chirp.times[data.com.chirp.idents == fish_id]\n        params = data.com.chirp.params[data.com.chirp.idents == fish_id]\n\n        for chirp, param in zip(chirps, params):\n            # take the two closest frequency points\n            f_closest = freqs[np.argsort(np.abs(times - chirp))[:2]]\n\n            # take the two closest time points\n            t_closest = times[np.argsort(np.abs(times - chirp))[:2]]\n\n            # compute the weighted average of the two closest frequency points\n            # using the dt between chirp time and sampled time as weights\n            f_closest = np.average(\n                f_closest,\n                weights=np.abs(t_closest - chirp),\n            )\n\n            # we now have baseline eodf and time point of the chirp. Now\n            # we get some parameters from the params to build the bounding box\n            # for the chirp\n            height = param[1]\n            width = param[2]\n\n            # now define bounding box as center coordinates, width and height\n            t_center = chirp\n            f_center = f_closest + height / 2\n\n            bbox_height = height + pad_freq\n            bbox_width = width + pad_time\n\n            boxes.append((t_center, f_center, bbox_width, bbox_height))\n            ids.append(fish_id)\n\n    dataframe = pd.DataFrame(\n        boxes,\n        columns=[\"t_center\", \"f_center\", \"width\", \"height\"],\n    )\n    dataframe[\"fish_id\"] = ids\n    return dataframe\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert","title":"<code>convert(data, conf, output, label_mode)</code>","text":"<p>Convert a gridtools dataset to a YOLO dataset.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert--parameters","title":"Parameters","text":"<ul> <li><code>data</code> : <code>Dataset</code>     The dataset to convert.</li> <li><code>conf</code> : <code>Config</code>     The configuration.</li> <li><code>output</code> : <code>pathlib.Path</code>     The output directory.</li> <li><code>label_mode</code> : <code>str</code>     The label mode. Can be one of 'none', 'synthetic' or 'detected'.</li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert--notes","title":"Notes","text":"<p>This function iterates through a raw recording in chunks and computes the sum spectrogram of each chunk. The chunk size needs to be chosen such that the images can be nicely fed to a detector. The function also computes the bounding boxes of chirps in that chunk and saves them to a dataframe and a txt file into a labels directory.</p> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def convert(\n    data: Dataset,\n    conf: Config,\n    output: pathlib.Path,\n    label_mode: str,\n) -&gt; None:\n    \"\"\"Convert a gridtools dataset to a YOLO dataset.\n\n    Parameters\n    ----------\n    - `data` : `Dataset`\n        The dataset to convert.\n    - `conf` : `Config`\n        The configuration.\n    - `output` : `pathlib.Path`\n        The output directory.\n    - `label_mode` : `str`\n        The label mode. Can be one of 'none', 'synthetic' or 'detected'.\n\n    Returns\n    -------\n    - `None`\n\n    Notes\n    -----\n    This function iterates through a raw recording in chunks and computes the\n    sum spectrogram of each chunk. The chunk size needs to be chosen such that\n    the images can be nicely fed to a detector. The function also computes\n    the bounding boxes of chirps in that chunk and saves them to a dataframe\n    and a txt file into a labels directory.\n    \"\"\"\n    assert hasattr(data, \"grid\"), \"Dataset must have a grid attribute\"\n    assert label_mode in [\n        \"none\",\n        \"synthetic\",\n        \"detected\",\n    ], \"label_mode must be one of 'none', 'synthetic' or 'detected'\"\n\n    dataroot = output\n\n    # How much time to put into each spectrogram\n    time_window = conf.spec.time_window  # seconds\n    window_overlap = conf.spec.spec_overlap  # seconds\n    freq_pad = conf.spec.freq_pad  # Hz\n    window_overlap_samples = int(window_overlap * data.grid.samplerate)\n    spectrogram_freq_limits = (\n        np.min(data.track.freqs) - freq_pad,\n        np.max(data.track.freqs) + freq_pad,\n    )\n\n    # Spectrogram computation parameters\n    nfft = freqres_to_nfft(conf.spec.freq_res, data.grid.samplerate)  # samples\n    hop_len = overlap_to_hoplen(conf.spec.overlap_frac, nfft)  # samples\n    chunksize = int(time_window * data.grid.samplerate)  # samples\n    n_chunks = np.ceil(data.grid.rec.shape[0] / chunksize).astype(int)\n    msg = (\n        \"Dividing recording of duration\"\n        f\"{data.grid.rec.shape[0] / data.grid.samplerate} into {n_chunks}\"\n        f\"chunks of {time_window} seconds each.\",\n    )\n    con.log(msg)\n\n    # stash here the dataframes with the bounding boxes\n    bbox_dfs = []\n\n    # shift the time of the tracks to start at 0\n    # because a subset starts at the orignal time\n    # TODO: Remove this when gridtools is fixed\n    data.track.times -= data.track.times[0]\n\n    for current_chunk in range(n_chunks):\n        # get start and stop indices for the current chunk\n        # including some overlap to compensate for edge effects\n        # this diffrers for the first and last chunk\n\n        idx1, idx2 = make_chunk_indices(\n            n_chunks,\n            current_chunk,\n            chunksize,\n            window_overlap_samples,\n            data.grid.rec.shape[0],\n        )\n\n        # idx1 and idx2 now determine the window I cut out of the raw signal\n        # to compute the spectrogram of.\n\n        # compute the time and frequency axes of the spectrogram now that we\n        # include the start and stop indices of the current chunk and thus the\n        # right start and stop time. The `spectrogram` function does not know\n        # about this and would start every time axis at 0.\n        spectrogram_times, spectrogram_frequencies = make_spectrogram_axes(\n            idx1,\n            idx2,\n            nfft,\n            hop_len,\n            data.grid.samplerate,\n        )\n\n        # If we reach the end of the recording, we need to cut off the last\n        # chunk at the end of the recording.\n\n        # make a subset of the current chunk\n        chunk = subset(data, idx1, idx2, mode=\"index\")\n\n        # compute the spectrogram for each electrode of the current chunk\n        spectrogram = compute_sum_spectrogam(chunk, nfft, hop_len)\n\n        # cut off everything outside the upper frequency limit\n        # the spec is still a tensor\n\n        spectrogram = spectrogram[\n            (spectrogram_frequencies &gt;= spectrogram_freq_limits[0])\n            &amp; (spectrogram_frequencies &lt;= spectrogram_freq_limits[1]),\n            :,\n        ]\n        spectrogram_frequencies = spectrogram_frequencies[\n            (spectrogram_frequencies &gt;= spectrogram_freq_limits[0])\n            &amp; (spectrogram_frequencies &lt;= spectrogram_freq_limits[1])\n        ]\n\n        # normalize the spectrogram to zero mean and unit variance\n        # the spec is still a tensor\n        spectrogram = zscore_standardize(spectrogram)\n\n        # convert the spectrogram to a PIL image and save\n        spectrogram = spectrogram.detach().cpu().numpy()\n        img = numpy_to_pil(spectrogram)\n        imgpath = dataroot / \"images\" / f\"{chunk.path.name}.png\"\n        img.save(imgpath)\n\n        if label_mode == \"synthetic\":\n            bboxes = bboxes_from_simulated_chirps(chunk, nfft)\n            bbox_df = convert_bboxes_from_simulated_chirps(\n                imgpath,\n                spectrogram_times,\n                spectrogram_frequencies,\n                current_chunk,\n                bboxes,\n            )\n            if bbox_df is None:\n                continue\n            bbox_dfs.append(bbox_df)\n            save_labels_for_simulated_chirps(bbox_df, dataroot)\n\n        elif label_mode == \"detected\":\n            detected_labels(\n                dataroot, chunk, imgpath.name, spectrogram, spectrogram_times\n            )\n\n    if label_mode == \"synthetic\":\n        bbox_df = pd.concat(bbox_dfs, ignore_index=True)\n        bbox_df.to_csv(dataroot / f\"{data.path.name}_bboxes.csv\", index=False)\n\n    # save the classes.txt file\n    classes = [\"__background__\", \"chirp\"]\n    with pathlib.Path.open(dataroot / \"classes.txt\", \"w\") as f:\n        f.write(\"\\n\".join(classes))\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert_bboxes_from_simulated_chirps","title":"<code>convert_bboxes_from_simulated_chirps(imgpath, spectrogram_times, spectrogram_frequencies, current_chunk, bboxes)</code>","text":"<p>Generate labels of a simulated dataset.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert_bboxes_from_simulated_chirps--parameters","title":"Parameters","text":"<ul> <li><code>imgpath</code> : <code>pathlib.Path</code>     The path to the image.</li> <li><code>spectrogram_times</code> : <code>np.ndarray</code>     The time axis of the spectrogram.</li> <li><code>spectrogram_frequencies</code> : <code>np.ndarray</code>     The frequency axis of the spectrogram.</li> <li><code>current_chunk</code> : <code>int</code>     The chunk number.</li> <li><code>bboxes</code> : <code>pd.DataFrame</code>     The bounding boxes.</li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert_bboxes_from_simulated_chirps--returns","title":"Returns","text":"<ul> <li><code>pandas.DataFrame</code>     A dataframe with the bounding boxes.</li> </ul> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def convert_bboxes_from_simulated_chirps(\n    imgpath: pathlib.Path,\n    spectrogram_times: np.ndarray,\n    spectrogram_frequencies: np.ndarray,\n    current_chunk: int,\n    bboxes: pd.DataFrame,\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Generate labels of a simulated dataset.\n\n    Parameters\n    ----------\n    - `imgpath` : `pathlib.Path`\n        The path to the image.\n    - `spectrogram_times` : `np.ndarray`\n        The time axis of the spectrogram.\n    - `spectrogram_frequencies` : `np.ndarray`\n        The frequency axis of the spectrogram.\n    - `current_chunk` : `int`\n        The chunk number.\n    - `bboxes` : `pd.DataFrame`\n        The bounding boxes.\n\n    Returns\n    -------\n    - `pandas.DataFrame`\n        A dataframe with the bounding boxes.\n    \"\"\"\n    # compute the bounding boxes for this chunk\n\n    if len(bboxes) == 0:\n        return None\n\n    # convert bounding box center coordinates to spectrogram coordinates\n    # find the indices on the spectrogram_times corresponding to the center\n    # times\n    x = np.searchsorted(spectrogram_times, bboxes.t_center)\n    y = np.searchsorted(spectrogram_frequencies, bboxes.f_center)\n    widths = np.searchsorted(\n        spectrogram_times - spectrogram_times[0], bboxes.width\n    )\n    heights = np.searchsorted(\n        spectrogram_frequencies - spectrogram_frequencies[0], bboxes.height\n    )\n\n    # now we have center coordinates, widths and heights in indices. But PIL\n    # expects coordinates in pixels in the format\n    # (Upper left x coordinate, upper left y coordinate,\n    # lower right x coordinate, lower right y coordinate)\n    # In addiotion, an image starts in the top left corner so the bboxes\n    # need to be mirrored horizontally.\n\n    y = len(spectrogram_frequencies) - y  # flip y values to fit y=0 at top\n    lxs, lys = x - widths / 2, y - heights / 2\n    rxs, rys = x + widths / 2, y + heights / 2\n\n    # add them to the bboxes dataframe\n    bboxes[\"upperleft_img_x\"] = lxs\n    bboxes[\"upperleft_img_y\"] = lys\n    bboxes[\"lowerright_img_x\"] = rxs\n    bboxes[\"lowerright_img_y\"] = rys\n\n    # yolo format is centerx, centery, width, height normalized to image size\n    # convert xmin, ymin, xmax, ymax to centerx, centery, width, height\n    centerx = (lxs + rxs) / 2\n    centery = (lys + rys) / 2\n    width = rxs - lxs\n    height = rys - lys\n\n    # most deep learning frameworks expect bounding box coordinates\n    # as relative to the image size. So we normalize the coordinates\n    # to the image size\n    centerx_norm = centerx / len(spectrogram_times)\n    centery_norm = centery / len(spectrogram_frequencies)\n    width_norm = width / len(spectrogram_times)\n    height_norm = height / len(spectrogram_frequencies)\n\n    # add them to the bboxes dataframe\n    # theses are the ones that will later make the label files\n    bboxes[\"centerx_norm\"] = centerx_norm\n    bboxes[\"centery_norm\"] = centery_norm\n    bboxes[\"width_norm\"] = width_norm\n    bboxes[\"height_norm\"] = height_norm\n\n    # add chunk ID to the bboxes dataframe\n    bboxes[\"chunk_id\"] = current_chunk\n\n    # add the image name to the bboxes dataframe\n    bboxes[\"image\"] = imgpath.name\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert_cli","title":"<code>convert_cli(path, output, label_mode)</code>","text":"<p>Parse all datasets in a directory and convert them to a YOLO dataset.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert_cli--parameters","title":"Parameters","text":"<ul> <li><code>path</code> : <code>pathlib.Path</code>     The root directory of the datasets.</li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.convert_cli--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def convert_cli(\n    path: pathlib.Path,\n    output: pathlib.Path,\n    label_mode: str,\n) -&gt; None:\n    \"\"\"Parse all datasets in a directory and convert them to a YOLO dataset.\n\n    Parameters\n    ----------\n    - `path` : `pathlib.Path`\n        The root directory of the datasets.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    make_file_tree(output)\n    config = load_config(str(path / \"chirpdetector.toml\"))\n\n    for p in track(list(path.iterdir()), description=\"Building datasets\"):\n        if p.is_file():\n            continue\n        data = load(p)\n        convert(data, config, output, label_mode)\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.detected_labels","title":"<code>detected_labels(output, chunk, imgname, spec, spectrogram_times)</code>","text":"<p>Use the detect_chirps to make a YOLO dataset.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.detected_labels--parameters","title":"Parameters","text":"<ul> <li><code>output</code> : <code>pathlib.Path</code>     The output directory.</li> <li><code>chunk</code> : <code>Dataset</code>     The dataset to make bounding boxes for.</li> <li><code>imgname</code> : <code>str</code>     The name of the image.</li> <li><code>spec</code> : <code>np.ndarray</code>     The spectrogram.</li> <li><code>spectrogram_times</code> : <code>np.ndarray</code>     The time axis of the spectrogram.</li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.detected_labels--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def detected_labels(\n    output: pathlib.Path,\n    chunk: Dataset,\n    imgname: str,\n    spec: np.ndarray,\n    spectrogram_times: np.ndarray,\n) -&gt; None:\n    \"\"\"Use the detect_chirps to make a YOLO dataset.\n\n    Parameters\n    ----------\n    - `output` : `pathlib.Path`\n        The output directory.\n    - `chunk` : `Dataset`\n        The dataset to make bounding boxes for.\n    - `imgname` : `str`\n        The name of the image.\n    - `spec` : `np.ndarray`\n        The spectrogram.\n    - `spectrogram_times` : `np.ndarray`\n        The time axis of the spectrogram.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    # load the detected bboxes csv\n    # TODO: This is a workaround. Instead improve the subset naming convention\n    # in gridtools\n    source_dataset = chunk.path.name.split(\"_\")[1:-4]\n    source_dataset = \"_\".join(source_dataset)\n    source_dataset = chunk.path.parent / source_dataset\n\n    dataframe = pd.read_csv(source_dataset / \"chirpdetector_bboxes.csv\")\n\n    # get chunk start and stop time\n    start, stop = spectrogram_times[0], spectrogram_times[-1]\n\n    # get the bboxes for this chunk\n    bboxes = dataframe[(dataframe.t1 &gt;= start) &amp; (dataframe.t2 &lt;= stop)]\n\n    # get the x and y coordinates of the bboxes in pixels as dataframe\n    bboxes_xy = bboxes[[\"x1\", \"y1\", \"x2\", \"y2\"]]\n\n    # convert from x1, y1, x2, y2 to centerx, centery, width, height\n    centerx = np.array((bboxes_xy[\"x1\"] + bboxes_xy[\"x2\"]) / 2)\n    centery = np.array((bboxes_xy[\"y1\"] + bboxes_xy[\"y2\"]) / 2)\n    width = np.array(bboxes_xy[\"x2\"] - bboxes_xy[\"x1\"])\n    height = np.array(bboxes_xy[\"y2\"] - bboxes_xy[\"y1\"])\n\n    # flip centery because origin is top left\n    centery = spec.shape[0] - centery\n\n    # make relative to image size\n    centerx = centerx / spec.shape[1]\n    centery = centery / spec.shape[0]\n    width = width / spec.shape[1]\n    height = height / spec.shape[0]\n    labels = np.ones_like(centerx, dtype=int)\n\n    # make a new dataframe with the relative coordinates\n    new_bboxes = pd.DataFrame(\n        {\"l\": labels, \"x\": centerx, \"y\": centery, \"w\": width, \"h\": height},\n    )\n\n    # save dataframe for every spec without headers as txt\n    new_bboxes.to_csv(\n        output / \"labels\" / f\"{imgname[:-4]}.txt\",\n        header=False,\n        index=False,\n        sep=\" \",\n    )\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.make_file_tree","title":"<code>make_file_tree(path)</code>","text":"<p>Build a file tree for the training dataset.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.make_file_tree--parameters","title":"Parameters","text":"<p>path : pathlib.Path     The root directory of the dataset.</p> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def make_file_tree(path: pathlib.Path) -&gt; None:\n    \"\"\"Build a file tree for the training dataset.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        The root directory of the dataset.\n    \"\"\"\n    if not isinstance(path, pathlib.Path):\n        msg = f\"Path must be a pathlib.Path, not {type(path)}\"\n        raise TypeError(msg)\n\n    if path.parent.exists() and path.parent.is_file():\n        msg = (\n            f\"Parent directory of {path} is a file. \"\n            \"Please specify a directory.\"\n        )\n        raise ValueError(msg)\n\n    if path.exists():\n        shutil.rmtree(path)\n\n    path.mkdir(exist_ok=True, parents=True)\n\n    train_imgs = path / \"images\"\n    train_labels = path / \"labels\"\n    train_imgs.mkdir(exist_ok=True, parents=True)\n    train_labels.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.numpy_to_pil","title":"<code>numpy_to_pil(img)</code>","text":"<p>Convert a 2D numpy array to a PIL image.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.numpy_to_pil--parameters","title":"Parameters","text":"<p>img : np.ndarray     The input image.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.numpy_to_pil--returns","title":"Returns","text":"<p>PIL.Image     The converted image.</p> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def numpy_to_pil(img: np.ndarray) -&gt; Image.Image:\n    \"\"\"Convert a 2D numpy array to a PIL image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        The input image.\n\n    Returns\n    -------\n    PIL.Image\n        The converted image.\n    \"\"\"\n    img_dimens = 2\n    if len(img.shape) != img_dimens:\n        msg = f\"Image must be {img_dimens}D\"\n        raise ValueError(msg)\n\n    if img.max() == img.min():\n        msg = \"Image must have more than one value\"\n        raise ValueError(msg)\n\n    img = np.flipud(img)\n    intimg = np.uint8((img - img.min()) / (img.max() - img.min()) * 255)\n    return Image.fromarray(intimg)\n</code></pre>"},{"location":"api/convert_data/#chirpdetector.convert_data.save_labels_for_simulated_chirps","title":"<code>save_labels_for_simulated_chirps(bbox_df, dataset_root)</code>","text":"<p>Save the labels for a simulated dataset.</p>"},{"location":"api/convert_data/#chirpdetector.convert_data.save_labels_for_simulated_chirps--parameters","title":"Parameters","text":"<ul> <li><code>bbox_df</code> : <code>pd.DataFrame</code>     The bounding boxes.</li> <li><code>dataset_root</code> : <code>pathlib.Path</code>     The root directory of the dataset.</li> </ul>"},{"location":"api/convert_data/#chirpdetector.convert_data.save_labels_for_simulated_chirps--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/convert_data.py</code> <pre><code>def save_labels_for_simulated_chirps(\n    bbox_df: pd.DataFrame, dataset_root: pathlib.Path\n) -&gt; None:\n    \"\"\"Save the labels for a simulated dataset.\n\n    Parameters\n    ----------\n    - `bbox_df` : `pd.DataFrame`\n        The bounding boxes.\n    - `dataset_root` : `pathlib.Path`\n        The root directory of the dataset.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    for img in bbox_df[\"image\"].unique():\n        x = bbox_df[\"centerx_norm\"].loc[bbox_df[\"image\"] == img]\n        y = bbox_df[\"centery_norm\"].loc[bbox_df[\"image\"] == img]\n        w = bbox_df[\"width_norm\"].loc[bbox_df[\"image\"] == img]\n        h = bbox_df[\"height_norm\"].loc[bbox_df[\"image\"] == img]\n\n        # make a dataframe with the labels\n        label_df = pd.DataFrame({\"cx\": x, \"cy\": y, \"w\": w, \"h\": h})\n        label_df.insert(0, \"instance_id\", np.ones_like(x, dtype=int))\n\n        # save dataframe for every spec without headers as txt\n        label_df.to_csv(\n            dataset_root / \"labels\" / f\"{img.stem}.txt\",\n            header=False,\n            index=False,\n            sep=\" \",\n        )\n</code></pre>"},{"location":"api/dataset_utils/","title":"dataset_utils","text":"<p>Utility functions for training datasets in the YOLO format.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.clean_yolo_dataset","title":"<code>clean_yolo_dataset(path, img_ext)</code>","text":"<p>Remove images and labels when the label file is empty.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.clean_yolo_dataset--parameters","title":"Parameters","text":"<p>path : pathlib.Path     The path to the dataset. img_ext : str</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.clean_yolo_dataset--returns","title":"Returns","text":"<p>None</p> Source code in <code>chirpdetector/dataset_utils.py</code> <pre><code>def clean_yolo_dataset(path: pathlib.Path, img_ext: str) -&gt; None:\n    \"\"\"Remove images and labels when the label file is empty.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        The path to the dataset.\n    img_ext : str\n\n    Returns\n    -------\n    None\n    \"\"\"\n    img_path = path / \"images\"\n    lbl_path = path / \"labels\"\n\n    images = list(img_path.glob(f\"*{img_ext}\"))\n\n    for image in images:\n        lbl = lbl_path / f\"{image.stem}.txt\"\n        if lbl.stat().st_size == 0:\n            image.unlink()\n            lbl.unlink()\n</code></pre>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.load_img","title":"<code>load_img(path)</code>","text":"<p>Load an image from a path as a numpy array.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.load_img--parameters","title":"Parameters","text":"<p>path : pathlib.Path     The path to the image.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.load_img--returns","title":"Returns","text":"<p>img : np.ndarray     The image as a numpy array.</p> Source code in <code>chirpdetector/dataset_utils.py</code> <pre><code>def load_img(path: pathlib.Path) -&gt; np.ndarray:\n    \"\"\"Load an image from a path as a numpy array.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        The path to the image.\n\n    Returns\n    -------\n    img : np.ndarray\n        The image as a numpy array.\n    \"\"\"\n    img = Image.open(path)\n    return np.asarray(img)\n</code></pre>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.merge_yolo_datasets","title":"<code>merge_yolo_datasets(dataset1, dataset2, output)</code>","text":"<p>Merge two yolo-style datasets into one.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.merge_yolo_datasets--parameters","title":"Parameters","text":"<p>dataset1 : str     The path to the first dataset. dataset2 : str     The path to the second dataset. output : str     The path to the output dataset.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.merge_yolo_datasets--returns","title":"Returns","text":"<p>None</p> Source code in <code>chirpdetector/dataset_utils.py</code> <pre><code>def merge_yolo_datasets(\n    dataset1: pathlib.Path,\n    dataset2: pathlib.Path,\n    output: pathlib.Path,\n) -&gt; None:\n    \"\"\"Merge two yolo-style datasets into one.\n\n    Parameters\n    ----------\n    dataset1 : str\n        The path to the first dataset.\n    dataset2 : str\n        The path to the second dataset.\n    output : str\n        The path to the output dataset.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    dataset1 = pathlib.Path(dataset1)\n    dataset2 = pathlib.Path(dataset2)\n    output = pathlib.Path(output)\n\n    if not dataset1.exists():\n        msg = f\"{dataset1} does not exist.\"\n        raise FileNotFoundError(msg)\n    if not dataset2.exists():\n        msg = f\"{dataset2} does not exist.\"\n        raise FileNotFoundError(msg)\n    if output.exists():\n        msg = f\"{output} already exists.\"\n        raise FileExistsError(msg)\n\n    output_images = output / \"images\"\n    output_images.mkdir(parents=True, exist_ok=False)\n    output_labels = output / \"labels\"\n    output_labels.mkdir(parents=True, exist_ok=False)\n\n    imgs1 = list((dataset1 / \"images\").iterdir())\n    labels1 = list((dataset1 / \"labels\").iterdir())\n    imgs2 = list((dataset2 / \"images\").iterdir())\n    labels2 = list((dataset2 / \"labels\").iterdir())\n\n    print(f\"Found {len(imgs1)} images in {dataset1}.\")\n    print(f\"Found {len(imgs2)} images in {dataset2}.\")\n\n    print(f\"Copying images and labels to {output}...\")\n    for idx, _ in enumerate(imgs1):\n        shutil.copy(imgs1[idx], output_images / imgs1[idx].name)\n        shutil.copy(labels1[idx], output_labels / labels1[idx].name)\n\n    for idx, _ in enumerate(imgs2):\n        shutil.copy(imgs2[idx], output_images / imgs2[idx].name)\n        shutil.copy(labels2[idx], output_labels / labels2[idx].name)\n\n    classes = dataset1 / \"classes.txt\"\n    shutil.copy(classes, output / classes.name)\n\n    print(f\"Done. Merged {len(imgs1) + len(imgs2)} images.\")\n</code></pre>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.plot_yolo_dataset","title":"<code>plot_yolo_dataset(path, n)</code>","text":"<p>Plot n random images YOLO-style dataset.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.plot_yolo_dataset--parameters","title":"Parameters","text":"<p>path : pathlib.Path     The path to the dataset.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.plot_yolo_dataset--returns","title":"Returns","text":"<p>None</p> Source code in <code>chirpdetector/dataset_utils.py</code> <pre><code>def plot_yolo_dataset(path: pathlib.Path, n: int) -&gt; None:\n    \"\"\"Plot n random images YOLO-style dataset.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        The path to the dataset.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    mpl.use(\"TkAgg\")\n    labelpath = path / \"labels\"\n    imgpath = path / \"images\"\n\n    label_paths = np.array(list(labelpath.glob(\"*.txt\")))\n    rng = np.random.default_rng()\n    label_paths = rng.choice(label_paths, size=n, replace=False)\n\n    for lp in label_paths:\n        imgp = imgpath / (lp.stem + \".png\")\n        img = load_img(imgp)\n        labs = np.loadtxt(lp, dtype=np.float32).reshape(-1, 5)\n\n        coords = labs[:, 1:]\n\n        # make coords absolute and normalize\n        coords[:, 0] *= img.shape[1]\n        coords[:, 1] *= img.shape[0]\n        coords[:, 2] *= img.shape[1]\n        coords[:, 3] *= img.shape[0]\n\n        # turn centerx, centery, width, height into xmin, ymin, xmax, ymax\n        xmin = coords[:, 0] - coords[:, 2] / 2\n        ymin = coords[:, 1] - coords[:, 3] / 2\n        xmax = coords[:, 0] + coords[:, 2] / 2\n        ymax = coords[:, 1] + coords[:, 3] / 2\n\n        # plot the image\n        _, ax = plt.subplots(figsize=(15, 5), constrained_layout=True)\n        ax.imshow(img, cmap=\"magma\")\n        for i in range(len(xmin)):\n            ax.add_patch(\n                Rectangle(\n                    (xmin[i], ymin[i]),\n                    xmax[i] - xmin[i],\n                    ymax[i] - ymin[i],\n                    fill=False,\n                    color=\"white\",\n                ),\n            )\n        ax.set_title(imgp.stem)\n        plt.axis(\"off\")\n        plt.show()\n</code></pre>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.subset_yolo_dataset","title":"<code>subset_yolo_dataset(path, img_ext, n)</code>","text":"<p>Subset a YOLO dataset.</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.subset_yolo_dataset--parameters","title":"Parameters","text":"<p>path : pathlib.Path     The path to the dataset root. img_ext : str     The image extension, e.g. .png or .jpg n : int     The size of the subset</p>"},{"location":"api/dataset_utils/#chirpdetector.dataset_utils.subset_yolo_dataset--returns","title":"Returns","text":"<p>None</p> Source code in <code>chirpdetector/dataset_utils.py</code> <pre><code>def subset_yolo_dataset(path: pathlib.Path, img_ext: str, n: int) -&gt; None:\n    \"\"\"Subset a YOLO dataset.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        The path to the dataset root.\n    img_ext : str\n        The image extension, e.g. .png or .jpg\n    n : int\n        The size of the subset\n\n    Returns\n    -------\n    None\n    \"\"\"\n    img_path = path / \"images\"\n    lbl_path = path / \"labels\"\n\n    images = np.array(img_path.glob(f\"*{img_ext}\"))\n    rng = np.random.default_rng()\n    images = rng.choice(images, size=len(images), replace=False)\n\n    images = images[:n]\n\n    subset_dir = path.parent / f\"{path.name}_subset\"\n    subset_dir.mkdir(exist_ok=True)\n\n    subset_img_path = subset_dir / \"images\"\n    subset_img_path.mkdir(exist_ok=True)\n    subset_lbl_path = subset_dir / \"labels\"\n    subset_lbl_path.mkdir(exist_ok=True)\n\n    shutil.copy(path / \"classes.txt\", subset_dir)\n\n    for image in images:\n        shutil.copy(image, subset_img_path)\n        shutil.copy(lbl_path / f\"{image.stem}.txt\", subset_lbl_path)\n</code></pre>"},{"location":"api/detect_chirps/","title":"detect_chirps","text":"<p>Detect chirps on a spectrogram.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.Timer","title":"<code>Timer</code>","text":"<p>A simple timer class.</p> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>class Timer:\n    \"\"\"A simple timer class.\"\"\"\n\n    def __enter__(self: Self) -&gt; Self:\n        \"\"\"Start the timer.\"\"\"\n        self.start_time = time.time()\n        return self\n\n    def __exit__(\n        self: Self, exc_type: str, exc_value: str, traceback: str\n    ) -&gt; None:\n        \"\"\"Stop the timer.\"\"\"\n        self.end_time = time.time()\n        self.execution_time = self.end_time - self.start_time\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.Timer.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the timer.</p> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def __enter__(self: Self) -&gt; Self:\n    \"\"\"Start the timer.\"\"\"\n    self.start_time = time.time()\n    return self\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.Timer.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Stop the timer.</p> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def __exit__(\n    self: Self, exc_type: str, exc_value: str, traceback: str\n) -&gt; None:\n    \"\"\"Stop the timer.\"\"\"\n    self.end_time = time.time()\n    self.execution_time = self.end_time - self.start_time\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.collect_specs","title":"<code>collect_specs(conf, data, t1)</code>","text":"<p>Collect the spectrograms of a dataset.</p> <p>Collec a batch of  sum spectrograms of a certain length (e.g. 15 seconds) for a dataset subset (e.g. of 90 seconds) depending on the power of the GPU.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.collect_specs--parameters","title":"Parameters","text":"<ul> <li><code>conf</code> : <code>Config</code>     The configuration object.</li> <li><code>data</code> : <code>Dataset</code>     The gridtools dataset to detect chirps on.</li> <li><code>t1</code> : <code>float</code>     The start time of the dataset.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.collect_specs--returns","title":"Returns","text":"<ul> <li><code>list</code>     The spectrograms.</li> <li><code>list</code>     The time axes of the spectrograms.</li> <li><code>list</code>     The frequency axes of the spectrograms.</li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def collect_specs(\n    conf: Config, data: Dataset, t1: float\n) -&gt; Tuple[list, list, list]:\n    \"\"\"Collect the spectrograms of a dataset.\n\n    Collec a batch of  sum spectrograms of a certain length (e.g. 15 seconds)\n    for a dataset subset (e.g. of 90 seconds) depending on the power of the\n    GPU.\n\n    Parameters\n    ----------\n    - `conf` : `Config`\n        The configuration object.\n    - `data` : `Dataset`\n        The gridtools dataset to detect chirps on.\n    - `t1` : `float`\n        The start time of the dataset.\n\n    Returns\n    -------\n    - `list`\n        The spectrograms.\n    - `list`\n        The time axes of the spectrograms.\n    - `list`\n        The frequency axes of the spectrograms.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    # make spec config\n    nfft = freqres_to_nfft(conf.spec.freq_res, data.grid.samplerate)  # samples\n    hop_len = overlap_to_hoplen(conf.spec.overlap_frac, nfft)  # samples\n    chunksize = int(conf.spec.time_window * data.grid.samplerate)  # samples\n    window_overlap_samples = int(conf.spec.spec_overlap * data.grid.samplerate)\n\n    # get the frequency limits of the spectrogram\n    flims = (\n        float(np.min(data.track.freqs) - conf.spec.freq_pad),\n        float(np.max(data.track.freqs) + conf.spec.freq_pad),\n    )\n\n    # make the start and stop indices for all chunks\n    # including some overlap to compensate for edge effects\n    idx1 = np.arange(\n        0,\n        data.grid.rec.shape[0] - chunksize,\n        chunksize - window_overlap_samples,\n    )\n    idx2 = idx1 + chunksize\n\n    # save data here\n    specs, times, freqs = [], [], []\n\n    # iterate over the chunks\n    for chunk_no, (start, stop) in enumerate(zip(idx1, idx2)):\n        # make a subset of for the current chunk\n        chunk = subset(data, start, stop, mode=\"index\")\n\n        # skip if there is no wavetracker tracking data in the current chunk\n        if len(chunk.track.indices) == 0:\n            continue\n\n        # compute the spectrogram for each electrode of the current chunk\n        with Timer() as t:\n            spec = compute_sum_spectrogam(chunk, nfft, hop_len)\n        msg = (\n            f\"Computing the sum spectrogram of chunk {chunk_no} took \"\n            f\"{t.execution_time:.2f} seconds.\"\n        )\n        prog.console.log(msg)\n        logger.debug(msg)\n\n        # compute the time and frequency axes of the spectrogam\n        spec_times, spec_freqs = make_spectrogram_axes(\n            start=start,\n            stop=stop,\n            nfft=nfft,\n            hop_length=hop_len,\n            samplerate=data.grid.samplerate,\n        )\n        spec_times += t1\n\n        # cut off everything outside the frequency limits\n        spec, spec_freqs = cut_spec_to_frequency_limits(\n            spec=spec,\n            spec_freqs=spec_freqs,\n            flims=flims,\n        )\n\n        # add the 3 channels, normalize to 0-1, etc\n        img = spec_to_image(spec)\n\n        # save the spectrogram\n        specs.append(img)\n        times.append(spec_times)\n        freqs.append(spec_freqs)\n\n    return specs, times, freqs\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.convert_model_output_to_df","title":"<code>convert_model_output_to_df(outputs, threshold, spec_times, spec_freqs)</code>","text":"<p>Convert the model output to a dataframe.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.convert_model_output_to_df--parameters","title":"Parameters","text":"<ul> <li><code>outputs</code> : <code>torch.Tensor</code>     The output of the model.</li> <li><code>threshold</code> : <code>float</code>     The threshold for the detections.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.convert_model_output_to_df--returns","title":"Returns","text":"<ul> <li><code>pandas.DataFrame</code>     The dataframe containing the bounding boxes.</li> <li><code>numpy.ndarray</code>     The scores of the detections.</li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def convert_model_output_to_df(\n    outputs: torch.Tensor,\n    threshold: float,\n    spec_times: list,\n    spec_freqs: list,\n) -&gt; Tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Convert the model output to a dataframe.\n\n    Parameters\n    ----------\n    - `outputs` : `torch.Tensor`\n        The output of the model.\n    - `threshold` : `float`\n        The threshold for the detections.\n\n    Returns\n    -------\n    - `pandas.DataFrame`\n        The dataframe containing the bounding boxes.\n    - `numpy.ndarray`\n        The scores of the detections.\n    \"\"\"\n    # put the boxes, scores and labels into the dataset\n\n    dfs = []\n    scores_out = []\n    for i in range(len(outputs)):\n        times = spec_times[i]\n        freqs = spec_freqs[i]\n        bboxes = outputs[i][\"boxes\"].detach().cpu().numpy()\n        scores = outputs[i][\"scores\"].detach().cpu().numpy()\n        labels = outputs[i][\"labels\"].detach().cpu().numpy()\n\n        # remove all boxes with a score below the threshold\n        bboxes = bboxes[scores &gt; threshold]\n        labels = labels[scores &gt; threshold]\n        scores = scores[scores &gt; threshold]\n\n        # save the bboxes to a dataframe\n        bbox_df = pd.DataFrame(\n            data=bboxes,\n            columns=[\"x1\", \"y1\", \"x2\", \"y2\"],\n        )\n        bbox_df[\"score\"] = scores\n        bbox_df[\"label\"] = labels\n\n        # convert the pixel coordinates to time and frequency\n        bbox_df = pixel_bbox_to_time_frequency(bbox_df, times, freqs)\n        scores_out.append(scores)\n        dfs.append(bbox_df)\n\n    dfs = pd.concat(dfs)\n    bbox_df = dfs.reset_index(drop=True)\n    scores = np.concatenate(scores_out)\n    return bbox_df, scores\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.coords_to_mpl_rectangle","title":"<code>coords_to_mpl_rectangle(boxes)</code>","text":"<p>Convert normal bounding box to matplotlib.pathes.Rectangle format.</p> <p>Convert box defined by corner coordinates (x1, y1, x2, y2) to box defined by lower left, width and height (x1, y1, w, h).</p> <p>The corner coordinates are the model output, but the center coordinates are needed by the <code>matplotlib.patches.Rectangle</code> object for plotting.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.coords_to_mpl_rectangle--parameters","title":"Parameters","text":"<ul> <li><code>boxes</code> : <code>numpy.ndarray</code>     The boxes to be converted.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.coords_to_mpl_rectangle--returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>     The converted boxes.</li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def coords_to_mpl_rectangle(boxes: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert normal bounding box to matplotlib.pathes.Rectangle format.\n\n    Convert box defined by corner coordinates (x1, y1, x2, y2)\n    to box defined by lower left, width and height (x1, y1, w, h).\n\n    The corner coordinates are the model output, but the center coordinates\n    are needed by the `matplotlib.patches.Rectangle` object for plotting.\n\n    Parameters\n    ----------\n    - `boxes` : `numpy.ndarray`\n        The boxes to be converted.\n\n    Returns\n    -------\n    - `numpy.ndarray`\n        The converted boxes.\n    \"\"\"\n    boxes_dims = 2\n    if len(boxes.shape) != boxes_dims:\n        msg = (\n            \"The boxes array must be 2-dimensional.\\n\"\n            f\"Shape of boxes: {boxes.shape}\"\n        )\n        raise ValueError(msg)\n    boxes_cols = 4\n    if boxes.shape[1] != boxes_cols:\n        msg = (\n            \"The boxes array must have 4 columns.\\n\"\n            f\"Shape of boxes: {boxes.shape}\"\n        )\n        raise ValueError(msg)\n\n    new_boxes = np.zeros_like(boxes)\n    new_boxes[:, 0] = boxes[:, 0]\n    new_boxes[:, 1] = boxes[:, 1]\n    new_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    new_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n    return new_boxes\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.cut_spec_to_frequency_limits","title":"<code>cut_spec_to_frequency_limits(spec, spec_freqs, flims)</code>","text":"<p>Cut off everything outside the frequency limits.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.cut_spec_to_frequency_limits--parameters","title":"Parameters","text":"<ul> <li><code>spec</code> : <code>torch.Tensor</code>     The spectrogram.</li> <li><code>spec_freqs</code> : <code>numpy.ndarray</code>     The frequency axis of the spectrogram.</li> <li><code>flims</code> : <code>tuple[float, float]</code>     The frequency limits.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.cut_spec_to_frequency_limits--returns","title":"Returns","text":"<ul> <li><code>torch.Tensor</code>     The cut spectrogram.</li> <li><code>numpy.ndarray</code>     The cut frequency axis.</li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def cut_spec_to_frequency_limits(\n    spec: torch.Tensor,\n    spec_freqs: np.ndarray,\n    flims: tuple[float, float],\n) -&gt; Tuple[torch.Tensor, np.ndarray]:\n    \"\"\"Cut off everything outside the frequency limits.\n\n    Parameters\n    ----------\n    - `spec` : `torch.Tensor`\n        The spectrogram.\n    - `spec_freqs` : `numpy.ndarray`\n        The frequency axis of the spectrogram.\n    - `flims` : `tuple[float, float]`\n        The frequency limits.\n\n    Returns\n    -------\n    - `torch.Tensor`\n        The cut spectrogram.\n    - `numpy.ndarray`\n        The cut frequency axis.\n    \"\"\"\n    # cut off everything outside the frequency limits\n    spec = spec[(spec_freqs &gt;= flims[0]) &amp; (spec_freqs &lt;= flims[1]), :]\n    spec_freqs = spec_freqs[\n        (spec_freqs &gt;= flims[0]) &amp; (spec_freqs &lt;= flims[1])\n    ]\n    return spec, spec_freqs\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.detect_chirps","title":"<code>detect_chirps(conf, data)</code>","text":"<p>Detect chirps on a spectrogram.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.detect_chirps--parameters","title":"Parameters","text":"<ul> <li><code>conf</code> : <code>Config</code>     The configuration object.</li> <li><code>data</code> : <code>Dataset</code>     The gridtools dataset to detect chirps on.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.detect_chirps--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def detect_chirps(conf: Config, data: Dataset) -&gt; None:\n    \"\"\"Detect chirps on a spectrogram.\n\n    Parameters\n    ----------\n    - `conf` : `Config`\n        The configuration object.\n    - `data` : `Dataset`\n        The gridtools dataset to detect chirps on.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    # load the model and the checkpoint, and set it to evaluation mode\n    logger = logging.getLogger(__name__)\n    device = get_device()\n    model = load_fasterrcnn(num_classes=len(conf.hyper.classes))\n    checkpoint = torch.load(\n        f\"{conf.hyper.modelpath}/model.pt\",\n        map_location=device,\n    )\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.to(device).eval()\n\n    window_duration = (\n        conf.spec.time_window * conf.spec.batch_size\n    ) - conf.spec.spec_overlap * (conf.spec.batch_size - 1)\n    window_duration_samples = int(window_duration * data.grid.samplerate)\n\n    msg = (\n        f\"Window duration: {window_duration:.2f} seconds.\\n\"\n        f\"Window duration in samples: {window_duration_samples} samples.\"\n    )\n    prog.console.log(msg)\n\n    # make start and stop indices for all chunks with overlap\n    window_overlap_samples = int(conf.spec.spec_overlap * data.grid.samplerate)\n    idx1 = np.arange(\n        0,\n        int(data.grid.rec.shape[0] - window_duration_samples),\n        int(window_duration_samples - window_overlap_samples),\n    )\n    idx2 = idx1 + window_duration_samples\n\n    # make a list to store the bboxes in for each chunk\n    bbox_dfs = []\n\n    msg = f\"Detecting chirps in {data.path.name}...\"\n    prog.console.log(msg)\n    logger.info(msg)\n\n    # iterate over the chunks\n    overwritten = False\n    for start, stop in zip(idx1, idx2):\n        total_batch_startt = time.time()\n\n        # make a subset of for the current chunk\n        with Timer() as t:\n            chunk = subset(data, start, stop, mode=\"index\")\n        msg = f\"Creating the batch subset took {t.execution_time:.2f} seconds.\"\n        prog.console.log(msg)\n\n        # skip if there is no wavetracker tracking data in the current chunk\n        if len(chunk.track.indices) == 0:\n            continue\n\n        # collect the spectrograms of the current batch\n        t1 = start / data.grid.samplerate\n        specs, spec_times, spec_freqs = collect_specs(conf, chunk, t1)\n\n        # perform the detection\n        with Timer() as t, torch.inference_mode():\n            outputs = model(specs)\n        msg = f\"Detection took {t.execution_time:.2f} seconds.\"\n        prog.console.log(msg)\n        logger.debug(msg)\n\n        # make a path to save the spectrogram\n        path = data.path / \"chirpdetections\"\n        if path.exists() and overwritten is False:\n            shutil.rmtree(path)\n            overwritten = True\n        path.mkdir(exist_ok=True)\n\n        # put the boxes, scores and labels into the dataset\n        bbox_df, scores = convert_model_output_to_df(\n            outputs, conf.det.threshold, spec_times, spec_freqs\n        )\n\n        num_chirps = len(scores[scores &gt; conf.det.threshold])\n        msg = f\"Number of chirps detected: {num_chirps}\"\n        prog.console.log(msg)\n\n        # Plot the spectrograms with bounding boxes (this is slow)\n        # startt = time.time()\n        # if np.any(scores &gt; conf.det.threshold):\n        #     for spec_no, (img, out) in enumerate(zip(specs,outputs)):\n        #         img_no  = chunk_no * len(specs) + spec_no\n        #         img_path = path / f\"cpd_detected_{img_no:05d}.png\"\n        #         plot_detections(\n        #             img, out, conf.det.threshold, img_path, conf\n        #         )\n        # endt = time.time()\n        # msg = f\"Plotting to disk took {endt - startt:.2f} seconds.\"\n        # prog.console.log(msg)\n        # logger.debug(msg)\n\n        # save df to list of dfs\n        bbox_dfs.append(bbox_df)\n        total_batch_endt = time.time()\n        msg = (\n            f\"Total batch processing time: \"\n            f\"{total_batch_endt - total_batch_startt:.2f} seconds.\\n\"\n        )\n        prog.console.log(msg)\n        prog.console.rule(\"Next batch\")\n        logger.debug(msg)\n\n    handle_dataframes(bbox_dfs, data.path)\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.detect_cli","title":"<code>detect_cli(input_path)</code>","text":"<p>Terminal interface for the detection function.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.detect_cli--parameters","title":"Parameters","text":"<ul> <li><code>path</code> : <code>str</code></li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.detect_cli--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def detect_cli(input_path: pathlib.Path) -&gt; None:\n    \"\"\"Terminal interface for the detection function.\n\n    Parameters\n    ----------\n    - `path` : `str`\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    logger = make_logger(__name__, input_path / \"chirpdetector.log\")\n    datasets = [folder for folder in input_path.iterdir() if folder.is_dir()]\n    confpath = input_path / \"chirpdetector.toml\"\n\n    # load the config file and print a warning if it does not exist\n    if confpath.exists():\n        config = load_config(str(confpath))\n    else:\n        msg = (\n            \"The configuration file could not be found in the specified path.\"\n            \"Please run `chirpdetector copyconfig` and change the \"\n            \"configuration file to your needs.\"\n        )\n        raise FileNotFoundError(msg)\n\n    # detect chirps in all datasets in the specified path\n    # and show a progress bar\n    prog.console.rule(\"Starting detection\")\n    logger.info(\"Starting detection -----------------------------------------\")\n    with prog:\n        task = prog.add_task(\"Detecting chirps...\", total=len(datasets))\n        for dataset in datasets:\n            startt = time.time()\n            data = load(dataset)\n            stopt = time.time()\n            msg = f\"Loading the dataset took {stopt - startt:.2f} seconds.\\n\"\n            prog.console.log(msg)\n            detect_chirps(config, data)\n            prog.update(task, advance=1)\n        prog.update(task, completed=len(datasets))\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.float_index_interpolation","title":"<code>float_index_interpolation(values, index_arr, data_arr)</code>","text":"<p>Convert float indices to values by linear interpolation.</p> <p>Interpolates a set of float indices within the given index array to obtain corresponding values from the data array using linear interpolation.</p> <p>Given a set of float indices (<code>values</code>), this function determines the corresponding values in the <code>data_arr</code> by linearly interpolating between adjacent indices in the <code>index_arr</code>. Linear interpolation involves calculating weighted averages based on the fractional parts of the float indices.</p> <p>This function is useful to transform float coordinates on a spectrogram matrix to the corresponding time and frequency values. The reason for this is, that the model outputs bounding boxes in float coordinates, i.e. it does not care about the exact pixel location of the bounding box.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.float_index_interpolation--parameters","title":"Parameters","text":"<ul> <li><code>values</code> : <code>np.ndarray</code>     The index value as a float that should be interpolated.</li> <li><code>index_arr</code> : <code>numpy.ndarray</code>     The array of indices on the data array.</li> <li><code>data_arr</code> : <code>numpy.ndarray</code>     The array of data.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.float_index_interpolation--returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>     The interpolated value.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.float_index_interpolation--raises","title":"Raises","text":"<ul> <li><code>ValueError</code>     If any of the input float indices (<code>values</code>) are outside     the range of the provided <code>index_arr</code>.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.float_index_interpolation--examples","title":"Examples","text":"<p>values = np.array([2.5, 3.2, 4.8]) index_arr = np.array([2, 3, 4, 5]) data_arr = np.array([10, 15, 20, 25]) result = float_index_interpolation(values, index_arr, data_arr) print(result) array([12.5, 16. , 22.5])</p> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def float_index_interpolation(\n    values: np.ndarray,\n    index_arr: np.ndarray,\n    data_arr: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Convert float indices to values by linear interpolation.\n\n    Interpolates a set of float indices within the given index\n    array to obtain corresponding values from the data\n    array using linear interpolation.\n\n    Given a set of float indices (`values`), this function determines\n    the corresponding values in the `data_arr` by linearly interpolating\n    between adjacent indices in the `index_arr`. Linear interpolation\n    involves calculating weighted averages based on the fractional\n    parts of the float indices.\n\n    This function is useful to transform float coordinates on a spectrogram\n    matrix to the corresponding time and frequency values. The reason for\n    this is, that the model outputs bounding boxes in float coordinates,\n    i.e. it does not care about the exact pixel location of the bounding\n    box.\n\n    Parameters\n    ----------\n    - `values` : `np.ndarray`\n        The index value as a float that should be interpolated.\n    - `index_arr` : `numpy.ndarray`\n        The array of indices on the data array.\n    - `data_arr` : `numpy.ndarray`\n        The array of data.\n\n    Returns\n    -------\n    - `numpy.ndarray`\n        The interpolated value.\n\n    Raises\n    ------\n    - `ValueError`\n        If any of the input float indices (`values`) are outside\n        the range of the provided `index_arr`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; values = np.array([2.5, 3.2, 4.8])\n    &gt;&gt;&gt; index_arr = np.array([2, 3, 4, 5])\n    &gt;&gt;&gt; data_arr = np.array([10, 15, 20, 25])\n    &gt;&gt;&gt; result = float_index_interpolation(values, index_arr, data_arr)\n    &gt;&gt;&gt; print(result)\n    array([12.5, 16. , 22.5])\n    \"\"\"\n    # Check if the values are within the range of the index array\n    if np.any(values &lt; (np.min(index_arr) - 1)) or np.any(\n        values &gt; (np.max(index_arr) + 1),\n    ):\n        msg = (\n            \"Values outside the range of index array\\n\"\n            f\"Target values: {values}\\n\"\n            f\"Index array: {index_arr}\\n\"\n            f\"Data array: {data_arr}\"\n        )\n        raise ValueError(msg)\n\n    # Find the indices corresponding to the values\n    lower_indices = np.floor(values).astype(int)\n    upper_indices = np.ceil(values).astype(int)\n\n    # Ensure upper indices are within the array bounds\n    upper_indices = np.minimum(upper_indices, len(index_arr) - 1)\n    lower_indices = np.minimum(lower_indices, len(index_arr) - 1)\n\n    # Calculate the interpolation weights\n    weights = values - lower_indices\n\n    # Linear interpolation\n    return (1 - weights) * data_arr[lower_indices] + weights * data_arr[\n        upper_indices\n    ]\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.handle_dataframes","title":"<code>handle_dataframes(bbox_dfs, output_path)</code>","text":"<p>Handle concatenation and saving of dataframes.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.handle_dataframes--parameters","title":"Parameters","text":"<ul> <li><code>bbox_dfs</code> : <code>list[pandas.DataFrame]</code>     The list of dataframes to concatenate.</li> <li><code>output_path</code> : <code>pathlib.Path</code>     The path to save the dataframe to.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.handle_dataframes--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def handle_dataframes(\n    bbox_dfs: list[pd.DataFrame], output_path: pathlib.Path\n) -&gt; None:\n    \"\"\"Handle concatenation and saving of dataframes.\n\n    Parameters\n    ----------\n    - `bbox_dfs` : `list[pandas.DataFrame]`\n        The list of dataframes to concatenate.\n    - `output_path` : `pathlib.Path`\n        The path to save the dataframe to.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    # concatenate all dataframes\n    bbox_df = pd.concat(bbox_dfs)\n    bbox_reset = bbox_df.reset_index(drop=True)\n\n    # sort the dataframe by t1\n    bbox_sorted = bbox_reset.sort_values(by=\"t1\")\n\n    # sort the columns\n    bbox_sorted = bbox_sorted[\n        [\"label\", \"score\", \"x1\", \"y1\", \"x2\", \"y2\", \"t1\", \"f1\", \"t2\", \"f2\"]\n    ]\n    # save the dataframe\n    bbox_sorted.to_csv(output_path / \"chirpdetector_bboxes.csv\", index=False)\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.pixel_bbox_to_time_frequency","title":"<code>pixel_bbox_to_time_frequency(bbox_df, spec_times, spec_freqs)</code>","text":"<p>Convert pixel coordinates to time and frequency.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.pixel_bbox_to_time_frequency--parameters","title":"Parameters","text":"<ul> <li><code>bbox_df</code> : <code>pandas.DataFrame</code>     The dataframe containing the bounding boxes.</li> <li><code>spec_times</code> : <code>numpy.ndarray</code>     The time axis of the spectrogram.</li> <li><code>spec_freqs</code> : <code>numpy.ndarray</code>     The frequency axis of the spectrogram.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.pixel_bbox_to_time_frequency--returns","title":"Returns","text":"<ul> <li><code>pandas.DataFrame</code>     The dataframe with the converted bounding boxes.</li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def pixel_bbox_to_time_frequency(\n    bbox_df: pd.DataFrame,\n    spec_times: np.ndarray,\n    spec_freqs: np.ndarray,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert pixel coordinates to time and frequency.\n\n    Parameters\n    ----------\n    - `bbox_df` : `pandas.DataFrame`\n        The dataframe containing the bounding boxes.\n    - `spec_times` : `numpy.ndarray`\n        The time axis of the spectrogram.\n    - `spec_freqs` : `numpy.ndarray`\n        The frequency axis of the spectrogram.\n\n    Returns\n    -------\n    - `pandas.DataFrame`\n        The dataframe with the converted bounding boxes.\n    \"\"\"\n    # convert x values to time on spec_times\n    spec_times_index = np.arange(0, len(spec_times))\n    bbox_df[\"t1\"] = float_index_interpolation(\n        bbox_df[\"x1\"].to_numpy(),\n        spec_times_index,\n        spec_times,\n    )\n    bbox_df[\"t2\"] = float_index_interpolation(\n        bbox_df[\"x2\"].to_numpy(),\n        spec_times_index,\n        spec_times,\n    )\n    # convert y values to frequency on spec_freqs\n    spec_freqs_index = np.arange(len(spec_freqs))\n    bbox_df[\"f1\"] = float_index_interpolation(\n        bbox_df[\"y1\"].to_numpy(),\n        spec_freqs_index,\n        spec_freqs,\n    )\n    bbox_df[\"f2\"] = float_index_interpolation(\n        bbox_df[\"y2\"].to_numpy(),\n        spec_freqs_index,\n        spec_freqs,\n    )\n    return bbox_df\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.plot_detections","title":"<code>plot_detections(img_tensor, output, threshold, save_path, conf)</code>","text":"<p>Plot the detections on the spectrogram.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.plot_detections--parameters","title":"Parameters","text":"<ul> <li><code>img_tensor</code> : <code>torch.Tensor</code>     The spectrogram.</li> <li><code>output</code> : <code>torch.Tensor</code>     The output of the model.</li> <li><code>threshold</code> : <code>float</code>     The threshold for the detections.</li> <li><code>save_path</code> : <code>pathlib.Path</code>     The path to save the plot to.</li> <li><code>conf</code> : <code>Config</code>     The configuration object.</li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.plot_detections--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def plot_detections(\n    img_tensor: torch.Tensor,\n    output: torch.Tensor,\n    threshold: float,\n    save_path: pathlib.Path,\n    conf: Config,\n) -&gt; None:\n    \"\"\"Plot the detections on the spectrogram.\n\n    Parameters\n    ----------\n    - `img_tensor` : `torch.Tensor`\n        The spectrogram.\n    - `output` : `torch.Tensor`\n        The output of the model.\n    - `threshold` : `float`\n        The threshold for the detections.\n    - `save_path` : `pathlib.Path`\n        The path to save the plot to.\n    - `conf` : `Config`\n        The configuration object.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    # retrieve all the data from the output and convert\n    # spectrogram to numpy array\n    img = img_tensor.detach().cpu().numpy().transpose(1, 2, 0)[..., 0]\n    boxes = output[\"boxes\"].detach().cpu().numpy()\n    boxes = coords_to_mpl_rectangle(boxes)\n    scores = output[\"scores\"].detach().cpu().numpy()\n    labels = output[\"labels\"].detach().cpu().numpy()\n    labels = [conf.hyper.classes[i] for i in labels]\n\n    _, ax = plt.subplots(figsize=(20, 10))\n\n    ax.pcolormesh(img, cmap=\"magma\")\n\n    for i, box in enumerate(boxes):\n        if scores[i] &gt; threshold:\n            ax.scatter(\n                box[0],\n                box[1],\n            )\n            ax.add_patch(\n                Rectangle(\n                    box[:2],\n                    box[2],\n                    box[3],\n                    fill=False,\n                    color=\"white\",\n                    linewidth=1,\n                ),\n            )\n            ax.text(\n                box[0],\n                box[1],\n                f\"{scores[i]:.2f}\",\n                color=\"black\",\n                fontsize=8,\n                bbox={\"facecolor\": \"white\", \"alpha\": 1},\n            )\n    plt.axis(\"off\")\n    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", pad_inches=0)\n    plt.close()\n</code></pre>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.spec_to_image","title":"<code>spec_to_image(spec)</code>","text":"<p>Convert a spectrogram to an image.</p> <p>Add 3 color channels, normalize to 0-1, etc.</p>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.spec_to_image--parameters","title":"Parameters","text":"<ul> <li><code>spec</code> : <code>torch.Tensor</code></li> </ul>"},{"location":"api/detect_chirps/#chirpdetector.detect_chirps.spec_to_image--returns","title":"Returns","text":"<ul> <li><code>torch.Tensor</code></li> </ul> Source code in <code>chirpdetector/detect_chirps.py</code> <pre><code>def spec_to_image(spec: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert a spectrogram to an image.\n\n    Add 3 color channels, normalize to 0-1, etc.\n\n    Parameters\n    ----------\n    - `spec` : `torch.Tensor`\n\n    Returns\n    -------\n    - `torch.Tensor`\n    \"\"\"\n    # make sure the spectrogram is a tensor\n    if not isinstance(spec, torch.Tensor):\n        msg = (\n            \"The spectrogram must be a torch.Tensor.\\n\"\n            f\"Type of spectrogram: {type(spec)}\"\n        )\n        raise TypeError(msg)\n\n    # make sure the spectrogram is 2-dimensional\n    spec_dims = 2\n    if len(spec.size()) != spec_dims:\n        msg = (\n            \"The spectrogram must be a 2-dimensional matrix.\\n\"\n            f\"Shape of spectrogram: {spec.size()}\"\n        )\n        raise ValueError(msg)\n\n    # make sure the spectrogram contains some data\n    if (\n        np.max(spec.detach().cpu().numpy())\n        - np.min(spec.detach().cpu().numpy())\n        == 0\n    ):\n        msg = (\n            \"The spectrogram must contain some data.\\n\"\n            f\"Max value: {np.max(spec.detach().cpu().numpy())}\\n\"\n            f\"Min value: {np.min(spec.detach().cpu().numpy())}\"\n        )\n        raise ValueError(msg)\n\n    # Get the dimensions of the original matrix\n    original_shape = spec.size()\n\n    # Calculate the number of rows and columns in the matrix\n    num_rows, num_cols = original_shape\n\n    # duplicate the matrix 3 times\n    spec = spec.repeat(3, 1, 1)\n\n    # Reshape the matrix to the desired shape (3, num_rows, num_cols)\n    desired_shape = (3, num_rows, num_cols)\n    reshaped_tensor = spec.view(desired_shape)\n\n    # normalize the spectrogram to be between 0 and 1\n    normalized_tensor = (reshaped_tensor - reshaped_tensor.min()) / (\n        reshaped_tensor.max() - reshaped_tensor.min()\n    )\n\n    # make sure image is float32\n    return normalized_tensor.float()\n</code></pre>"},{"location":"api/plot_detections/","title":"plot_detections","text":"<p>Functions to visualize detections on images.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.Spectrogram","title":"<code>Spectrogram</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class to store spectrogram parameters.</p> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>class Spectrogram(BaseModel):\n    \"\"\"Class to store spectrogram parameters.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    powers: np.ndarray\n    freqs: np.ndarray\n    times: np.ndarray\n</code></pre>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.clean_all_plots_cli","title":"<code>clean_all_plots_cli(path)</code>","text":"<p>Remove all plots from the chirpdetections folder.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.clean_all_plots_cli--parameters","title":"Parameters","text":"<p>path : pathlib.Path     Path to the config file.</p> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>def clean_all_plots_cli(path: pathlib.Path) -&gt; None:\n    \"\"\"Remove all plots from the chirpdetections folder.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        Path to the config file.\n    \"\"\"\n    dirs = [dataset for dataset in path.iterdir() if dataset.is_dir()]\n    with prog:\n        task = prog.add_task(\"Cleaning plots...\", total=len(dirs))\n        for dataset in dirs:\n            prog.console.log(f\"Cleaning plots for {dataset.name}\")\n            clean_plots_cli(dataset)\n            prog.advance(task)\n</code></pre>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.clean_plots_cli","title":"<code>clean_plots_cli(path)</code>","text":"<p>Remove all plots from the chirpdetections folder.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.clean_plots_cli--parameters","title":"Parameters","text":"<p>path : pathlib.Path     Path to the config file.</p> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>def clean_plots_cli(path: pathlib.Path) -&gt; None:\n    \"\"\"Remove all plots from the chirpdetections folder.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        Path to the config file.\n    \"\"\"\n    savepath = path / \"chirpdetections\"\n    for f in savepath.iterdir():\n        f.unlink()\n</code></pre>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_all_detections","title":"<code>plot_all_detections(data, chirp_df, conf)</code>","text":"<p>Plot all chirp detections of a full recording on spectrograms.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_all_detections--parameters","title":"Parameters","text":"<p>data : Dataset     The dataset. chirp_df : pd.DataFrame     The dataframe containing the chirp detections. conf : Config     The config file.</p> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>def plot_all_detections(\n    data: Dataset,\n    chirp_df: pd.DataFrame,\n    conf: Config,\n) -&gt; None:\n    \"\"\"Plot all chirp detections of a full recording on spectrograms.\n\n    Parameters\n    ----------\n    data : Dataset\n        The dataset.\n    chirp_df : pd.DataFrame\n        The dataframe containing the chirp detections.\n    conf : Config\n        The config file.\n    \"\"\"\n    time_window = 15\n    nfft = freqres_to_nfft(conf.spec.freq_res, data.grid.samplerate)  # samples\n    hop_len = overlap_to_hoplen(conf.spec.overlap_frac, nfft)  # samples\n    chunksize = int(time_window * data.grid.samplerate)  # samples\n    nchunks = np.ceil(data.grid.rec.shape[0] / chunksize).astype(int)\n    window_overlap_samples = int(conf.spec.spec_overlap * data.grid.samplerate)\n\n    # Set y limits for the spectrogram\n    flims = (\n        np.min(data.track.freqs) - 200,\n        np.max(data.track.freqs) + 700,\n    )\n\n    for chunk_no in range(nchunks):\n        # get start and stop indices for the current chunk\n        # including some overlap to compensate for edge effects\n        # this diffrers for the first and last chunk\n        idx1, idx2 = make_chunk_indices(\n            n_chunks=nchunks,\n            current_chunk=chunk_no,\n            chunksize=chunksize,\n            window_overlap_samples=window_overlap_samples,\n            max_end=data.grid.rec.shape[0],\n        )\n\n        # subset the data to the current chunk\n        chunk = subset(data, idx1, idx2, mode=\"index\")\n        chunk.track.times += idx1 / data.grid.samplerate\n\n        # dont plot chunks without chirps\n        if len(chunk.com.chirp.times) == 0:\n            continue\n\n        if len(chunk.track.indices) == 0:\n            continue\n\n        # compute the time and frequency axes of the spectrogram\n        spec_times, spec_freqs = make_spectrogram_axes(\n            start=idx1,\n            stop=idx2,\n            nfft=nfft,\n            hop_length=hop_len,\n            samplerate=data.grid.samplerate,\n        )\n        # compute the spectrogram for each electrode of the current chunk\n        spec = compute_sum_spectrogam(\n            data=chunk,\n            nfft=nfft,\n            hop_len=hop_len,\n        )\n\n        # detach from GPU and convert to numpy\n        spec = spec.detach().cpu().numpy()\n        spec = spec[(spec_freqs &gt;= flims[0]) &amp; (spec_freqs &lt;= flims[1]), :]\n        spec_freqs = spec_freqs[\n            (spec_freqs &gt;= flims[0]) &amp; (spec_freqs &lt;= flims[1])\n        ]\n        spectrogram = Spectrogram(\n            powers=spec,\n            freqs=spec_freqs,\n            times=spec_times,\n        )\n\n        # Extract the bounding boxes for the current chunk\n        chunk_t1 = idx1 / data.grid.samplerate\n        chunk_t2 = idx2 / data.grid.samplerate\n        chunk_df = chirp_df[\n            (chirp_df[\"t1\"] &gt;= chunk_t1) &amp; (chirp_df[\"t2\"] &lt;= chunk_t2)\n        ]\n\n        # get t1, t2, f1, f2 from chunk_df\n        bboxes = chunk_df[[\"score\", \"t1\", \"f1\", \"t2\", \"f2\"]].to_numpy()\n\n        plot_detections(\n            data=data,\n            spec=spectrogram,\n            bboxes=bboxes,\n            save_path=data.path,\n            file_index=f\"{chunk_no:05d}\",\n        )\n</code></pre>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_all_detections_cli","title":"<code>plot_all_detections_cli(path)</code>","text":"<p>Plot detections on images.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_all_detections_cli--parameters","title":"Parameters","text":"<p>path : pathlib.Path     Path to the config file.</p> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>def plot_all_detections_cli(path: pathlib.Path) -&gt; None:\n    \"\"\"Plot detections on images.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        Path to the config file.\n    \"\"\"\n    conf = load_config(path / \"chirpdetector.toml\")\n\n    dirs = [dataset for dataset in path.iterdir() if dataset.is_dir()]\n    with prog:\n        task = prog.add_task(\"Plotting detections...\", total=len(dirs))\n        for dataset in dirs:\n            prog.console.log(f\"Plotting detections for {dataset.name}\")\n            data = load(dataset)\n            chirp_df = pd.read_csv(dataset / \"chirpdetector_bboxes.csv\")\n            plot_all_detections(data, chirp_df, conf)\n            prog.advance(task)\n</code></pre>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_detections","title":"<code>plot_detections(data, spec, bboxes, save_path, file_index)</code>","text":"<p>Plot a spectrogram with tracks, bounding boxes and chirp times.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_detections--parameters","title":"Parameters","text":"<ul> <li><code>data</code> : <code>Dataset</code>     The dataset from gridtools.datasets.</li> <li><code>spec</code> : <code>Spectrogram</code>     The spectrogram.</li> <li><code>bboxes</code> : <code>np.ndarray</code>     The bounding boxes, as returned by <code>chirpdetector</code>.</li> <li><code>file_index</code> : <code>str</code>     The file index, used for saving the plot.</li> </ul>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_detections--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>def plot_detections(\n    data: Dataset,\n    spec: Spectrogram,\n    bboxes: np.ndarray,\n    save_path: pathlib.Path,\n    file_index: str,\n) -&gt; None:\n    \"\"\"Plot a spectrogram with tracks, bounding boxes and chirp times.\n\n    Parameters\n    ----------\n    - `data` : `Dataset`\n        The dataset from gridtools.datasets.\n    - `spec` : `Spectrogram`\n        The spectrogram.\n    - `bboxes` : `np.ndarray`\n        The bounding boxes, as returned by `chirpdetector`.\n    - `file_index` : `str`\n        The file index, used for saving the plot.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    _, ax = plt.subplots(figsize=(10, 5), constrained_layout=True)\n\n    # plot the spectrogram\n    extent = (\n        spec.times[0],\n        spec.times[-1],\n        spec.freqs[0],\n        spec.freqs[-1],\n    )\n    ax.imshow(\n        spec.powers,\n        aspect=\"auto\",\n        origin=\"lower\",\n        interpolation=\"gaussian\",\n        extent=extent,\n        cmap=\"magma\",\n    )\n\n    # plot the bounding boxes\n    for bbox in bboxes:\n        ax.add_patch(\n            Rectangle(\n                (bbox[1], bbox[2]),\n                bbox[3] - bbox[1],\n                bbox[4] - bbox[2],\n                fill=False,\n                color=\"gray\",\n                linewidth=1,\n                label=\"faster-R-CNN predictions\",\n            ),\n        )\n        ax.text(\n            bbox[1],\n            bbox[4] + 15,\n            f\"{bbox[0]:.2f}\",\n            color=\"gray\",\n            fontsize=10,\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            rotation=90,\n        )\n\n    # plot chirp times and frequency traces\n    for track_id in np.unique(data.track.idents):\n        chirptimes = data.com.chirp.times[data.com.chirp.idents == track_id]\n        freqs = data.track.freqs[data.track.idents == track_id]\n        times = data.track.times[\n            data.track.indices[data.track.idents == track_id]\n        ]\n        freqs = freqs[(times &gt;= extent[0] - 10) &amp; (times &lt;= extent[1] + 10)]\n        times = times[(times &gt;= extent[0] - 10) &amp; (times &lt;= extent[1] + 10)]\n\n        # get freqs where times are closest to ctimes\n        chirp_eodfs = np.zeros_like(chirptimes)\n        for i, ctime in enumerate(chirptimes):\n            try:\n                indx = np.argmin(np.abs(times - ctime))\n                chirp_eodfs[i] = freqs[indx]\n            except ValueError:\n                msg = (\n                    \"Failed to find track time closest to chirp time \"\n                    f\"in chunk {file_index}, check the plots.\"\n                )\n                prog.console.log(msg)\n\n        if len(times) != 0:\n            ax.plot(\n                times,\n                freqs,\n                lw=2,\n                color=\"black\",\n                label=\"Frequency traces\",\n            )\n        ax.scatter(\n            chirptimes,\n            chirp_eodfs,\n            marker=\"o\",\n            lw=1,\n            facecolor=\"white\",\n            edgecolor=\"black\",\n            s=25,\n            zorder=10,\n            label=\"Chirp assignments\",\n        )\n\n    ax.set_ylim(spec.freqs[0] + 5, spec.freqs[-1] - 5)\n    ax.set_xlim([spec.times[0], spec.times[-1]])\n\n    ax.set_xlabel(\"Time [s]\", fontsize=12)\n    ax.set_ylabel(\"Frequency [Hz]\", fontsize=12)\n\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    plt.legend(\n        by_label.values(),\n        by_label.keys(),\n        bbox_to_anchor=(0.5, 1.02),\n        loc=\"lower center\",\n        mode=\"None\",\n        borderaxespad=0,\n        ncol=3,\n        fancybox=False,\n        framealpha=0,\n    )\n\n    save_path = (\n        save_path / \"chirpdetections\" / f\"cpd_assigned_{file_index}.png\"\n    )\n    save_path.parent.mkdir(exist_ok=True)\n    plt.savefig(\n        save_path,\n        dpi=300,\n        bbox_inches=\"tight\",\n    )\n\n    plt.close()\n    plt.clf()\n    plt.cla()\n    plt.close(\"all\")\n</code></pre>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_detections_cli","title":"<code>plot_detections_cli(path)</code>","text":"<p>Plot detections on images.</p>"},{"location":"api/plot_detections/#chirpdetector.plot_detections.plot_detections_cli--parameters","title":"Parameters","text":"<p>path : pathlib.Path     Path to the config file.</p> Source code in <code>chirpdetector/plot_detections.py</code> <pre><code>def plot_detections_cli(path: pathlib.Path) -&gt; None:\n    \"\"\"Plot detections on images.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        Path to the config file.\n    \"\"\"\n    conf = load_config(path.parent / \"chirpdetector.toml\")\n    data = load(path)\n    chirp_df = pd.read_csv(path / \"chirpdetector_bboxes.csv\")\n    plot_all_detections(data, chirp_df, conf)\n</code></pre>"},{"location":"api/train_model/","title":"train_model","text":"<p>Train and test the neural network.</p>"},{"location":"api/train_model/#chirpdetector.train_model.FoldMetrics","title":"<code>FoldMetrics</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metrics for each fold.</p>"},{"location":"api/train_model/#chirpdetector.train_model.FoldMetrics--parameters","title":"Parameters","text":"<ul> <li><code>n_epochs</code>: <code>int</code>     How many epochs were trained.</li> <li><code>iou_thresholds</code>: <code>List[float]</code>     Which IoU thresholds were used to compute the models performance     metrics.</li> <li><code>avg_train_loss</code>: <code>List[float]</code>     The average training loss for each epoch.</li> <li><code>avg_val_loss</code>: <code>List[float]</code>     The average validation loss for each epoch.</li> <li><code>metrics</code>: <code>List[List[PerformanceMetrics]]</code>     The performance metrics for each epoch for each IoU threshold.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>class FoldMetrics(BaseModel):\n    \"\"\"Metrics for each fold.\n\n    Parameters\n    ----------\n    - `n_epochs`: `int`\n        How many epochs were trained.\n    - `iou_thresholds`: `List[float]`\n        Which IoU thresholds were used to compute the models performance\n        metrics.\n    - `avg_train_loss`: `List[float]`\n        The average training loss for each epoch.\n    - `avg_val_loss`: `List[float]`\n        The average validation loss for each epoch.\n    - `metrics`: `List[List[PerformanceMetrics]]`\n        The performance metrics for each epoch for each IoU threshold.\n    \"\"\"\n\n    n_epochs: int  # how many epochs were trained\n    iou_thresholds: List[float]  # which IoU thresholds were used\n    avg_train_loss: List[float]  # average training loss per epoch\n    avg_val_loss: List[float]  # average validation loss per epoch\n    metrics: List[\n        List[PerformanceMetrics]\n    ]  # metrics (eg AP) per epoch per IoU\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.PerformanceMetrics","title":"<code>PerformanceMetrics</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Performance metrics for object detection models.</p>"},{"location":"api/train_model/#chirpdetector.train_model.PerformanceMetrics--parameters","title":"Parameters","text":"<ul> <li><code>classes</code>: <code>List[int]</code>     The classes.</li> <li><code>precision</code>: <code>List[List[float]]</code>     The precision per class per target (bbox).</li> <li><code>recall</code>: <code>List[List[float]]</code>     The recall per class per target (bbox).</li> <li><code>f1</code>: <code>List[List[float]]</code>     The f1 score per class per target (bbox).</li> <li><code>scores</code>: <code>List[List[float]]</code>     The scores per class per target (bbox).</li> <li><code>average_precision</code>: <code>List[float]</code>     The average precision per class.</li> <li><code>mean_avg_prec</code>: <code>float</code>     The mean average precision.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>class PerformanceMetrics(BaseModel):\n    \"\"\"Performance metrics for object detection models.\n\n    Parameters\n    ----------\n    - `classes`: `List[int]`\n        The classes.\n    - `precision`: `List[List[float]]`\n        The precision per class per target (bbox).\n    - `recall`: `List[List[float]]`\n        The recall per class per target (bbox).\n    - `f1`: `List[List[float]]`\n        The f1 score per class per target (bbox).\n    - `scores`: `List[List[float]]`\n        The scores per class per target (bbox).\n    - `average_precision`: `List[float]`\n        The average precision per class.\n    - `mean_avg_prec`: `float`\n        The mean average precision.\n    \"\"\"\n\n    classes: List[int]  # list of classes, here only one class\n    precision: List[List[float]]  # precision per class per target (bbox)\n    recall: List[List[float]]  # recall per class per target (bbox)\n    f1: List[List[float]]  # f1 score per class per target (bbox)\n    scores: List[List[float]]  # scores per class per target (bbox)\n\n    average_precision: List[float]  # average precision per class\n    mean_avg_prec: float  # mean average precision\n    checkpoint: bool = False  # whether this is a checkpoint\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.bbox_dict_to_list","title":"<code>bbox_dict_to_list(predicted_bboxes, groundtruth_bboxes)</code>","text":"<p>Convert the predicted and groundtruth bboxes to lists.</p> <p>Format will be as follows: [img_idx, label, score, x1, y1, x2, y2]</p> <p>For ground truth the score will always be 1.</p>"},{"location":"api/train_model/#chirpdetector.train_model.bbox_dict_to_list--parameters","title":"Parameters","text":"<ul> <li><code>predicted_bboxes</code>: <code>List[dict]</code>     The predicted bboxes.</li> <li><code>groundtruth_bboxes</code>: <code>List[dict]</code>     The groundtruth bboxes.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.bbox_dict_to_list--returns","title":"Returns","text":"<ul> <li><code>pred_boxes</code>: <code>List</code>     The predicted bboxes.</li> <li><code>true_boxes</code>: <code>List</code>     The groundtruth bboxes.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def bbox_dict_to_list(\n    predicted_bboxes: List[dict],\n    groundtruth_bboxes: List[dict],\n) -&gt; Tuple[List[List], List[List]]:\n    \"\"\"Convert the predicted and groundtruth bboxes to lists.\n\n    Format will be as follows:\n    [img_idx, label, score, x1, y1, x2, y2]\n\n    For ground truth the score will always be 1.\n\n    Parameters\n    ----------\n    - `predicted_bboxes`: `List[dict]`\n        The predicted bboxes.\n    - `groundtruth_bboxes`: `List[dict]`\n        The groundtruth bboxes.\n\n    Returns\n    -------\n    - `pred_boxes`: `List`\n        The predicted bboxes.\n    - `true_boxes`: `List`\n        The groundtruth bboxes.\n    \"\"\"\n    pred_boxes = []\n    true_boxes = []\n    for img_idx in range(len(predicted_bboxes)):\n        for bbox_idx in range(len(predicted_bboxes[img_idx][\"boxes\"])):\n            # gather data of predictions\n            pred_img = img_idx\n            pred_label = (\n                predicted_bboxes[img_idx][\"labels\"][bbox_idx].cpu().numpy()\n            )\n            pred_score = (\n                predicted_bboxes[img_idx][\"scores\"][bbox_idx].cpu().numpy()\n            )\n            pred_bbox = (\n                predicted_bboxes[img_idx][\"boxes\"][bbox_idx].cpu().numpy()\n            )\n\n            # collapse all remaining dimensions\n            pred_label = collapse_all_dims(pred_label).tolist()\n            pred_score = collapse_all_dims(pred_score).tolist()\n            pred_bbox = collapse_all_dims(pred_bbox).tolist()\n\n            pred_total_bbox = [pred_img, pred_label, pred_score]\n            pred_total_bbox.extend(pred_bbox)\n            pred_boxes.append(pred_total_bbox)\n\n        for bbox_idx in range(len(groundtruth_bboxes[img_idx][\"boxes\"])):\n            # gather data of groundtruth\n            true_img = img_idx\n            true_label = (\n                groundtruth_bboxes[img_idx][\"labels\"][bbox_idx].cpu().numpy()\n            )\n            true_score = 1\n            true_bbox = (\n                groundtruth_bboxes[img_idx][\"boxes\"][bbox_idx].cpu().numpy()\n            )\n\n            # collapse all remaining dimensions\n            true_label = collapse_all_dims(true_label).tolist()\n            true_bbox = collapse_all_dims(true_bbox).tolist()\n\n            true_total_bbox = [true_img, true_label, true_score]\n            true_total_bbox.extend(true_bbox)\n            true_boxes.append(true_total_bbox)\n\n    return pred_boxes, true_boxes\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.collapse_all_dims","title":"<code>collapse_all_dims(arr)</code>","text":"<p>Collapse all dimensions of an array.</p>"},{"location":"api/train_model/#chirpdetector.train_model.collapse_all_dims--parameters","title":"Parameters","text":"<ul> <li><code>np.ndarray</code>: <code>np.ndarray</code>     The array to collapse.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.collapse_all_dims--returns","title":"Returns","text":"<ul> <li><code>np.ndarray</code>     The collapsed array.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def collapse_all_dims(arr: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Collapse all dimensions of an array.\n\n    Parameters\n    ----------\n    - `np.ndarray`: `np.ndarray`\n        The array to collapse.\n\n    Returns\n    -------\n    - `np.ndarray`\n        The collapsed array.\n    \"\"\"\n    while len(np.shape(arr)) &gt; 1:\n        arr = np.squeeze(arr)\n\n    return arr\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.intersection_over_union","title":"<code>intersection_over_union(boxes_preds, boxes_labels, box_format='corners')</code>","text":"<p>Calculate intersection over union.</p> <p>Adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master /ML/Pytorch/object_detection/metrics/iou.py</p>"},{"location":"api/train_model/#chirpdetector.train_model.intersection_over_union--parameters","title":"Parameters","text":"<pre><code>boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\nboxes_labels (tensor): Correct Labels of Boxes (BATCH_SIZE, 4)\nbox_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.intersection_over_union--returns","title":"Returns","text":"<pre><code>tensor: Intersection over union for all examples\n</code></pre> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def intersection_over_union(\n    boxes_preds: torch.Tensor,\n    boxes_labels: torch.Tensor,\n    box_format: str = \"corners\",\n) -&gt; torch.Tensor:\n    \"\"\"Calculate intersection over union.\n\n    Adapted from:\n    https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master\n    /ML/Pytorch/object_detection/metrics/iou.py\n\n    Parameters\n    ----------\n        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n        boxes_labels (tensor): Correct Labels of Boxes (BATCH_SIZE, 4)\n        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n\n    Returns\n    -------\n        tensor: Intersection over union for all examples\n    \"\"\"\n    if box_format not in [\"midpoint\", \"corners\"]:\n        msg = (\n            f\"Unknown box format {box_format}. Must be one of: \"\n            \"'midpoint', 'corners'.\"\n        )\n        raise ValueError(msg)\n\n    # Slicing idx:idx+1 in order to keep tensor dimensionality\n    # Doing ... in indexing if there would be additional dimensions\n    # Like for Yolo algorithm which would have (N, S, S, 4) in shape\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n    elif box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n    else:\n        msg = \"Provided box format is correct but failed to compute boxes.\"\n        raise ValueError(msg)\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    # Need clamp(0) in case they do not intersect, then we want intersection\n    # to be 0\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.mean_average_precision","title":"<code>mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format='corners', num_classes=1)</code>","text":"<p>Calculate mean average precision and metrics used in it.</p> <p>Adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection /blob/master/ML/Pytorch/object_detection/metrics/mean_avg_precision.py</p>"},{"location":"api/train_model/#chirpdetector.train_model.mean_average_precision--parameters","title":"Parameters","text":"<ul> <li><code>pred_boxes</code> : <code>list</code>     list of lists containing all bboxes with each bboxes     specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]</li> <li><code>true_boxes</code> : <code>list</code>     Similar as pred_boxes except all the correct ones. Score is set to 1.</li> <li><code>iou_threshold</code> : <code>float</code>     IOU threshold where predicted bboxes is correct. See     intersection_over_union function.</li> <li><code>box_format</code> : <code>str</code>     \"midpoint\" or \"corners\" used to specify bboxes     Midpoint is YOLO format: [x, y, width, height] and corners is     e.g. COCO format: [x1, y1, x2, y2]. This model outputs     \"corners\" format.</li> <li><code>num_classes</code> : <code>int</code>     number of classes</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.mean_average_precision--returns","title":"Returns","text":"<ul> <li><code>PerformanceMetrics</code>     The performance metrics.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def mean_average_precision(  # noqa\n    pred_boxes: list,\n    true_boxes: list,\n    iou_threshold: float = 0.5,\n    box_format: str = \"corners\",\n    num_classes: int = 1,\n) -&gt; PerformanceMetrics:\n    \"\"\"Calculate mean average precision and metrics used in it.\n\n    Adapted from:\n    https://github.com/aladdinpersson/Machine-Learning-Collection\n    /blob/master/ML/Pytorch/object_detection/metrics/mean_avg_precision.py\n\n    Parameters\n    ----------\n    - `pred_boxes` : `list`\n        list of lists containing all bboxes with each bboxes\n        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n    - `true_boxes` : `list`\n        Similar as pred_boxes except all the correct ones. Score is set to 1.\n    - `iou_threshold` : `float`\n        IOU threshold where predicted bboxes is correct. See\n        intersection_over_union function.\n    - `box_format` : `str`\n        \"midpoint\" or \"corners\" used to specify bboxes\n        Midpoint is YOLO format: [x, y, width, height] and corners is\n        e.g. COCO format: [x1, y1, x2, y2]. This model outputs\n        \"corners\" format.\n    - `num_classes` : `int`\n        number of classes\n\n    Returns\n    -------\n    - `PerformanceMetrics`\n        The performance metrics.\n    \"\"\"\n    # list storing all AP for respective classes\n    all_scores = []\n    all_precisions = []\n    all_recalls = []\n    all_f1 = []\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    # __background__ is a reserved class so there is no class 0\n    # in the model output\n    classes = np.arange(num_classes) + 1\n    classes = classes.tolist()\n\n    # This function is created for multiclass but in this\n    # case we have only one class\n    # So this is not actually the mean average precision\n    # but just the average precision\n    for c in classes:\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n\n        # Make zeros for every detection and then later set\n        # it to 1 if it is a true positive or false positive\n        TP = torch.zeros((len(detections)))  # noqa\n        FP = torch.zeros((len(detections)))  # noqa\n\n        # Collect the number of total true bboxes for this class\n        total_true_bboxes = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        # Otherwise we now loob over each prediction\n        # and get the corresponding ground truth\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            best_iou = 0\n            best_gt_idx = None\n\n            # Now get the ground truth bbox that has the highest\n            # iou with this detection bbox\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou &gt; best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            # Once we're done looking for the best match,\n            # we check if the IoU is greater than the threshold\n            # If it is then it's a true positive, else a false positive\n            if best_iou &gt; iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n\n                # if the bounding box has already been detected,\n                # it is a false positive so a duplicate\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        # Compute how many true positives and false positives we have\n        TP_cumsum = torch.cumsum(TP, dim=0)  # noqa\n        FP_cumsum = torch.cumsum(FP, dim=0)  # noqa\n\n        # Compute corresponding detection scores\n        scores = torch.tensor([detection[2] for detection in detections])\n        scores = torch.cat((torch.tensor([1]), scores))\n        all_scores.append(scores.tolist())\n\n        # Compute precision and recall\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        all_recalls.append(recalls.tolist())\n        all_precisions.append(precisions.tolist())\n\n        # We need to 1 to the precision so that numerical integration\n        # is correct.\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n\n        # compute the f1 scores\n        f1 = 2 * (precisions * recalls) / (precisions + recalls + epsilon)\n        all_f1.append(f1.tolist())\n\n        # Integral of prec-rec curve: torch.trapz for numerical integration\n        # (AP = area under prec-rec curve)\n        avg_prec = torch.trapz(precisions, recalls)\n        average_precisions.append(avg_prec.item())\n\n    # mean average precision is the mean of all the average precisions\n    mean_avg_prec = sum(average_precisions) / len(average_precisions)\n    mean_avg_prec = float(mean_avg_prec)\n\n    # instantiate the performance metrics\n    return PerformanceMetrics(\n        classes=classes,\n        precision=all_precisions,\n        recall=all_recalls,\n        f1=all_f1,\n        scores=all_scores,\n        average_precision=average_precisions,\n        mean_avg_prec=mean_avg_prec,\n    )\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.plot_epochs","title":"<code>plot_epochs(epoch_train_loss, epoch_val_loss, epoch_avg_train_loss, epoch_avg_val_loss, path)</code>","text":"<p>Plot the loss for each epoch.</p>"},{"location":"api/train_model/#chirpdetector.train_model.plot_epochs--parameters","title":"Parameters","text":"<ul> <li><code>epoch_train_loss</code>: <code>list</code>     The training loss for each epoch.</li> <li><code>epoch_val_loss</code>: <code>list</code>     The validation loss for each epoch.</li> <li><code>epoch_avg_train_loss</code>: <code>list</code>     The average training loss for each epoch.</li> <li><code>epoch_avg_val_loss</code>: <code>list</code>     The average validation loss for each epoch.</li> <li><code>path</code>: <code>pathlib.Path</code>     The path to save the plot to.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.plot_epochs--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def plot_epochs(\n    epoch_train_loss: list,\n    epoch_val_loss: list,\n    epoch_avg_train_loss: list,\n    epoch_avg_val_loss: list,\n    path: pathlib.Path,\n) -&gt; None:\n    \"\"\"Plot the loss for each epoch.\n\n    Parameters\n    ----------\n    - `epoch_train_loss`: `list`\n        The training loss for each epoch.\n    - `epoch_val_loss`: `list`\n        The validation loss for each epoch.\n    - `epoch_avg_train_loss`: `list`\n        The average training loss for each epoch.\n    - `epoch_avg_val_loss`: `list`\n        The average validation loss for each epoch.\n    - `path`: `pathlib.Path`\n        The path to save the plot to.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    _, ax = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True)\n\n    x_train = np.arange(len(epoch_train_loss[0])) + 1\n    x_val = np.arange(len(epoch_val_loss[0])) + len(epoch_train_loss[0]) + 1\n\n    for train_loss, val_loss in zip(epoch_train_loss, epoch_val_loss):\n        ax[0].plot(x_train, train_loss, c=\"tab:blue\", label=\"_\")\n        ax[0].plot(x_val, val_loss, c=\"tab:orange\", label=\"_\")\n        x_train = np.arange(len(epoch_train_loss[0])) + x_val[-1]\n        x_val = np.arange(len(epoch_val_loss[0])) + x_train[-1]\n\n    x_avg = np.arange(len(epoch_avg_train_loss)) + 1\n    ax[1].plot(\n        x_avg,\n        epoch_avg_train_loss,\n        label=\"Training Loss\",\n        c=\"tab:blue\",\n    )\n    ax[1].plot(\n        x_avg,\n        epoch_avg_val_loss,\n        label=\"Validation Loss\",\n        c=\"tab:orange\",\n    )\n\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Batch\")\n    ax[0].set_ylim(bottom=0)\n    ax[0].set_title(\"Loss per batch\")\n\n    ax[1].set_ylabel(\"Loss\")\n    ax[1].set_xlabel(\"Epoch\")\n    ax[1].legend()\n    ax[1].set_ylim(bottom=0)\n    ax[1].set_title(\"Avg loss per epoch\")\n\n    plt.savefig(path)\n    plt.close()\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.plot_folds","title":"<code>plot_folds(fold_avg_train_loss, fold_avg_val_loss, path)</code>","text":"<p>Plot the loss for each fold.</p>"},{"location":"api/train_model/#chirpdetector.train_model.plot_folds--parameters","title":"Parameters","text":"<ul> <li><code>fold_avg_train_loss</code>: <code>list</code>     The average training loss for each fold.</li> <li><code>fold_avg_val_loss</code>: <code>list</code>     The average validation loss for each fold.</li> <li><code>path</code>: <code>pathlib.Path</code>     The path to save the plot to.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.plot_folds--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def plot_folds(\n    fold_avg_train_loss: list,\n    fold_avg_val_loss: list,\n    path: pathlib.Path,\n) -&gt; None:\n    \"\"\"Plot the loss for each fold.\n\n    Parameters\n    ----------\n    - `fold_avg_train_loss`: `list`\n        The average training loss for each fold.\n    - `fold_avg_val_loss`: `list`\n        The average validation loss for each fold.\n    - `path`: `pathlib.Path`\n        The path to save the plot to.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    _, ax = plt.subplots(figsize=(10, 5), constrained_layout=True)\n\n    for train_loss, val_loss in zip(fold_avg_train_loss, fold_avg_val_loss):\n        x = np.arange(len(train_loss)) + 1\n        ax.plot(x, train_loss, c=\"tab:blue\", alpha=0.3, label=\"_\")\n        ax.plot(x, val_loss, c=\"tab:orange\", alpha=0.3, label=\"_\")\n\n    avg_train = np.mean(fold_avg_train_loss, axis=0)\n    avg_val = np.mean(fold_avg_val_loss, axis=0)\n    x = np.arange(len(avg_train)) + 1\n    ax.plot(\n        x,\n        avg_train,\n        label=\"Training Loss\",\n        c=\"tab:blue\",\n    )\n    ax.plot(\n        x,\n        avg_val,\n        label=\"Validation Loss\",\n        c=\"tab:orange\",\n    )\n\n    ax.set_ylabel(\"Loss\")\n    ax.set_xlabel(\"Epoch\")\n    ax.legend()\n    ax.set_ylim(bottom=0)\n\n    plt.savefig(path)\n    plt.close()\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.save_model","title":"<code>save_model(epoch, model, optimizer, path)</code>","text":"<p>Save the model state dict.</p>"},{"location":"api/train_model/#chirpdetector.train_model.save_model--parameters","title":"Parameters","text":"<ul> <li><code>epoch</code>: <code>int</code>     The current epoch.</li> <li><code>model</code>: <code>torch.nn.Module</code>     The model to save.</li> <li><code>optimizer</code>: <code>torch.optim.Optimizer</code>     The optimizer to save.</li> <li><code>path</code>: <code>pathlib.Path</code>     The path to save the model to.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.save_model--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def save_model(\n    epoch: int,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    path: pathlib.Path,\n) -&gt; None:\n    \"\"\"Save the model state dict.\n\n    Parameters\n    ----------\n    - `epoch`: `int`\n        The current epoch.\n    - `model`: `torch.nn.Module`\n        The model to save.\n    - `optimizer`: `torch.optim.Optimizer`\n        The optimizer to save.\n    - `path`: `pathlib.Path`\n        The path to save the model to.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    path = pathlib.Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        },\n        path / \"model.pt\",\n    )\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.train","title":"<code>train(config, mode='pretrain')</code>","text":"<p>Train the model.</p>"},{"location":"api/train_model/#chirpdetector.train_model.train--parameters","title":"Parameters","text":"<ul> <li><code>config</code>: <code>Config</code>     The config file.</li> <li><code>mode</code>: <code>str</code>     The mode to train in. Either <code>pretrain</code> or <code>finetune</code>.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.train--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def train(config: Config, mode: str = \"pretrain\") -&gt; None:  # noqa\n    \"\"\"Train the model.\n\n    Parameters\n    ----------\n    - `config`: `Config`\n        The config file.\n    - `mode`: `str`\n        The mode to train in. Either `pretrain` or `finetune`.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    # Load a pretrained model from pytorch if in pretrain mode,\n    # otherwise open an already trained model from the\n    # model state dict.\n    if mode == \"pretrain\":\n        assert config.train.datapath is not None\n        datapath = config.train.datapath\n    elif mode == \"finetune\":\n        assert config.finetune.datapath is not None\n        datapath = config.finetune.datapath\n    else:\n        msg = f\"Unknown mode {mode}. Must be one of: 'pretrain', 'finetune'.\"\n        raise ValueError(msg)\n\n    # Check if the path to the data actually exists\n    if not pathlib.Path(datapath).exists():\n        msg = f\"Path {datapath} does not exist.\"\n        raise FileNotFoundError(msg)\n\n    # Initialize the logger and progress bar, make the logger global\n    logger = make_logger(\n        __name__,\n        pathlib.Path(config.path).parent / \"chirpdetector.log\",\n    )\n\n    # Get the device (e.g. GPU or CPU)\n    device = get_device()\n\n    # Print information about starting training\n    progress.console.rule(\"Starting training\")\n    msg = (\n        f\"Device: {device}, Config: {config.path},\"\n        f\" Mode: {mode}, Data: {datapath}\"\n    )\n    progress.console.log(msg)\n    logger.info(msg)\n\n    # initialize the dataset\n    data = CustomDataset(\n        path=datapath,\n        classes=config.hyper.classes,\n    )\n\n    # initialize the k-fold cross-validation\n    splits = KFold(n_splits=config.hyper.kfolds, shuffle=True, random_state=42)\n\n    # initialize the IoU threshold sweep for the validation\n    iou_thresholds = np.round(np.arange(0.5, 1.0, 0.05), 2)\n\n    # initialize the best validation loss to a large number\n    best_val_loss = float(\"inf\")\n\n    # iterate over the folds for k-fold cross-validation\n    with progress:\n        # save loss across all epochs and folds\n        fold_train_loss = []\n        fold_val_loss = []\n        fold_avg_train_loss = []\n        fold_avg_val_loss = []\n\n        # Add kfolds progress bar that runs alongside the epochs progress bar\n        task_folds = progress.add_task(\n            f\"[blue]{config.hyper.kfolds}-Fold Crossvalidation\",\n            total=config.hyper.kfolds,\n        )\n\n        # iterate over the folds\n        for fold, (train_idx, val_idx) in enumerate(\n            splits.split(np.arange(len(data))),\n        ):\n            # initialize the model and optimizer\n            model = load_fasterrcnn(num_classes=len(config.hyper.classes)).to(\n                device,\n            )\n\n            # If the mode is finetune, load the model state dict from\n            # previous training\n            if mode == \"finetune\":\n                modelpath = pathlib.Path(config.hyper.modelpath) / \"model.pt\"\n                checkpoint = torch.load(modelpath, map_location=device)\n                model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n            # Initialize stochastic gradient descent optimizer\n            params = [p for p in model.parameters() if p.requires_grad]\n            optimizer = torch.optim.SGD(\n                params,\n                lr=config.hyper.learning_rate,\n                momentum=config.hyper.momentum,\n                weight_decay=config.hyper.weight_decay,\n            )\n\n            # make train and validation dataloaders for the current fold\n            train_data = torch.utils.data.Subset(data, train_idx)\n            val_data = torch.utils.data.Subset(data, val_idx)\n\n            # this is for training\n            train_loader = DataLoader(\n                train_data,\n                batch_size=config.hyper.batch_size,\n                shuffle=True,\n                num_workers=config.hyper.num_workers,\n                collate_fn=collate_fn,\n            )\n\n            # this is only for validation\n            val_loader = DataLoader(\n                val_data,\n                batch_size=config.hyper.batch_size,\n                shuffle=True,\n                num_workers=config.hyper.num_workers,\n                collate_fn=collate_fn,\n            )\n\n            # save loss across all epochs\n            epoch_avg_train_loss = []\n            epoch_avg_val_loss = []\n            epoch_train_loss = []\n            epoch_val_loss = []\n            epoch_metrics = []\n\n            # train the model for the specified number of epochs\n            task_epochs = progress.add_task(\n                f\"{config.hyper.num_epochs} Epochs for fold k={fold + 1}\",\n                total=config.hyper.num_epochs,\n            )\n\n            # iterate across n epochs\n            for epoch in range(config.hyper.num_epochs):\n                # print information about the current epoch\n                msg = (\n                    f\"Training epoch {epoch + 1} of {config.hyper.num_epochs} \"\n                    f\"for fold {fold + 1} of {config.hyper.kfolds}\"\n                )\n                progress.console.log(msg)\n                logger.info(msg)\n\n                # train the epoch\n                train_loss = train_epoch(\n                    dataloader=train_loader,\n                    device=device,\n                    model=model,\n                    optimizer=optimizer,\n                )\n\n                # validate the epoch\n                val_loss, predicted_bboxes, true_bboxes = val_epoch(\n                    dataloader=val_loader,\n                    device=device,\n                    model=model,\n                )\n\n                # Compute model performance metrics\n                # for this epoch\n                metric_sweep = []\n                for iou_threshold in iou_thresholds:\n                    metrics = mean_average_precision(\n                        pred_boxes=predicted_bboxes,\n                        true_boxes=true_bboxes,\n                        iou_threshold=iou_threshold,\n                        box_format=\"corners\",\n                        num_classes=1,\n                    )\n                    metric_sweep.append(metrics)\n\n                # save losses for this epoch\n                epoch_train_loss.append(train_loss)\n                epoch_val_loss.append(val_loss)\n\n                # save the average loss for this epoch\n                epoch_avg_train_loss.append(np.median(train_loss))\n                epoch_avg_val_loss.append(np.median(val_loss))\n\n                # save the model if it is the best so far\n                if np.mean(val_loss) &lt; best_val_loss:\n                    best_val_loss = sum(val_loss) / len(val_loss)\n\n                    # get the mean average precision\n                    ap = np.mean(\n                        [metric.mean_avg_prec for metric in metric_sweep]\n                    )\n\n                    # write checkpoint to the metrics\n                    for metric in metric_sweep:\n                        metric.checkpoint = True\n\n                    msg = (\n                        f\"New best validation loss: {best_val_loss:.4f}, \\n\"\n                        f\"Current AP@[.50:.05:.95] {ap:.4f}, \\n\"\n                        \"saving model...\"\n                    )\n                    progress.console.log(msg)\n                    logger.info(msg)\n\n                    modelpath = pathlib.Path(config.hyper.modelpath)\n                    save_model(\n                        epoch=epoch,\n                        model=model,\n                        optimizer=optimizer,\n                        path=modelpath,\n                    )\n\n                # save the metrics for this epoch\n                epoch_metrics.append(metric_sweep)\n\n                # plot the losses for this epoch\n                plot_epochs(\n                    epoch_train_loss=epoch_train_loss,\n                    epoch_val_loss=epoch_val_loss,\n                    epoch_avg_train_loss=epoch_avg_train_loss,\n                    epoch_avg_val_loss=epoch_avg_val_loss,\n                    path=pathlib.Path(config.hyper.modelpath)\n                    / f\"fold{fold + 1}.png\",\n                )\n\n                # update the progress bar for the epochs\n                progress.update(task_epochs, advance=1)\n\n            # update the progress bar for the epochs and hide it if done\n            progress.update(task_epochs, visible=False)\n\n            # save the losses for this fold\n            fold_train_loss.append(epoch_train_loss)\n            fold_val_loss.append(epoch_val_loss)\n            fold_avg_train_loss.append(epoch_avg_train_loss)\n            fold_avg_val_loss.append(epoch_avg_val_loss)\n\n            plot_folds(\n                fold_avg_train_loss=fold_avg_train_loss,\n                fold_avg_val_loss=fold_avg_val_loss,\n                path=pathlib.Path(config.hyper.modelpath) / \"losses.png\",\n            )\n\n            # save the losses for this fold\n            fold_metrics = FoldMetrics(\n                n_epochs=config.hyper.num_epochs,\n                iou_thresholds=iou_thresholds.tolist(),\n                avg_train_loss=epoch_avg_train_loss,\n                avg_val_loss=epoch_avg_val_loss,\n                metrics=epoch_metrics,\n            )\n\n            filename = f\"fold_{fold + 1}.json\"\n            filepath = pathlib.Path(config.hyper.modelpath) / filename\n            with filepath.open(\"w\") as outfile:\n                json.dump(fold_metrics.dict(), outfile)\n\n            # update the progress bar for the folds\n            progress.update(task_folds, advance=1)\n\n        # update the progress bar for the folds and hide it if done\n        progress.update(task_folds, visible=False)\n\n        # print information about the training\n        msg = (\n            \"Average validation loss of last epoch across folds: \"\n            f\"{np.mean(fold_val_loss):.4f}\"\n        )\n        progress.console.log(msg)\n        logger.info(msg)\n        progress.console.rule(\"[bold blue]Finished training\")\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.train_cli","title":"<code>train_cli(config_path, mode)</code>","text":"<p>Train the model from the command line.</p>"},{"location":"api/train_model/#chirpdetector.train_model.train_cli--parameters","title":"Parameters","text":"<ul> <li><code>config_path</code>: <code>pathlib.Path</code>     The path to the config file.</li> <li><code>mode</code>: <code>str</code>     The mode to train in. Either <code>pretrain</code> or <code>finetune</code>.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.train_cli--returns","title":"Returns","text":"<ul> <li><code>None</code></li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def train_cli(config_path: pathlib.Path, mode: str) -&gt; None:\n    \"\"\"Train the model from the command line.\n\n    Parameters\n    ----------\n    - `config_path`: `pathlib.Path`\n        The path to the config file.\n    - `mode`: `str`\n        The mode to train in. Either `pretrain` or `finetune`.\n\n    Returns\n    -------\n    - `None`\n    \"\"\"\n    config = load_config(config_path)\n    train(config, mode=mode)\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.train_epoch","title":"<code>train_epoch(dataloader, device, model, optimizer)</code>","text":"<p>Train the model for one epoch.</p>"},{"location":"api/train_model/#chirpdetector.train_model.train_epoch--parameters","title":"Parameters","text":"<ul> <li><code>dataloader</code>: <code>DataLoader</code>     The dataloader for the training data.</li> <li><code>device</code>: <code>torch.device</code>     The device to train on.</li> <li><code>model</code>: <code>torch.nn.Module</code>     The model to train.</li> <li><code>optimizer</code>: <code>torch.optim.Optimizer</code>     The optimizer to use.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.train_epoch--returns","title":"Returns","text":"<ul> <li><code>train_loss</code>: <code>List</code>     The training loss for each batch.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def train_epoch(\n    dataloader: DataLoader,\n    device: torch.device,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n) -&gt; List:\n    \"\"\"Train the model for one epoch.\n\n    Parameters\n    ----------\n    - `dataloader`: `DataLoader`\n        The dataloader for the training data.\n    - `device`: `torch.device`\n        The device to train on.\n    - `model`: `torch.nn.Module`\n        The model to train.\n    - `optimizer`: `torch.optim.Optimizer`\n        The optimizer to use.\n\n    Returns\n    -------\n    - `train_loss`: `List`\n        The training loss for each batch.\n    \"\"\"\n    train_loss = []\n\n    for samples, targets in dataloader:\n        images = [sample.to(device) for sample in samples]\n        bboxes = [\n            {k: v.to(device) for k, v in t.items() if k != \"image_name\"}\n            for t in targets\n        ]\n\n        model.train()\n        loss_dict = model(images, bboxes)\n        losses = sum(loss for loss in loss_dict.values())\n        train_loss.append(losses.item())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n    return train_loss\n</code></pre>"},{"location":"api/train_model/#chirpdetector.train_model.val_epoch","title":"<code>val_epoch(dataloader, device, model)</code>","text":"<p>Validate the model for one epoch.</p>"},{"location":"api/train_model/#chirpdetector.train_model.val_epoch--parameters","title":"Parameters","text":"<ul> <li><code>dataloader</code>: <code>DataLoader</code>     The dataloader for the validation data.</li> <li><code>device</code>: <code>torch.device</code>     The device to train on.</li> <li><code>model</code>: <code>torch.nn.Module</code>     The model to train.</li> </ul>"},{"location":"api/train_model/#chirpdetector.train_model.val_epoch--returns","title":"Returns","text":"<ul> <li><code>loss_dict</code>: <code>dict</code>     The loss dictionary.</li> </ul> Source code in <code>chirpdetector/train_model.py</code> <pre><code>def val_epoch(\n    dataloader: DataLoader,\n    device: torch.device,\n    model: torch.nn.Module,\n) -&gt; Tuple[List, List, List]:\n    \"\"\"Validate the model for one epoch.\n\n    Parameters\n    ----------\n    - `dataloader`: `DataLoader`\n        The dataloader for the validation data.\n    - `device`: `torch.device`\n        The device to train on.\n    - `model`: `torch.nn.Module`\n        The model to train.\n\n    Returns\n    -------\n    - `loss_dict`: `dict`\n        The loss dictionary.\n    \"\"\"\n    val_loss = []\n    groundtruth_bboxes = []\n    predicted_bboxes = []\n    for samples, targets in dataloader:\n        images = [sample.to(device) for sample in samples]\n        bboxes = [\n            {k: v.to(device) for k, v in t.items() if k != \"image_name\"}\n            for t in targets\n        ]\n\n        # get losses for each image\n        with torch.inference_mode():\n            # to get the loss_dict\n            model.train()\n            loss_dict = model(images, bboxes)\n\n            # to get the predicted boxes\n            model.eval()\n            predicted_bboxes.extend(model(images))\n\n        losses = sum(loss for loss in loss_dict.values())\n        groundtruth_bboxes.extend(bboxes)\n        val_loss.append(losses.item())\n\n    pred_boxes, true_boxes = bbox_dict_to_list(\n        predicted_bboxes, groundtruth_bboxes\n    )\n\n    return val_loss, pred_boxes, true_boxes\n</code></pre>"}]}